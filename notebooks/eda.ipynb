{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Project Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kedro \n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set Kedro project path\n",
    "project_path = Path.cwd().parent\n",
    "\n",
    "# Bootstrap Kedro\n",
    "from kedro.framework.startup import bootstrap_project\n",
    "from kedro.framework.session import KedroSession\n",
    "\n",
    "bootstrap_project(project_path)\n",
    "session = KedroSession.create(project_path=project_path)\n",
    "context = session.load_context()\n",
    "catalog = context.catalog\n",
    "\n",
    "# Add src/ to Python path\n",
    "sys.path.append(str(project_path / \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import full modules (for reload)\n",
    "import egt305_job_market_analysis.utils.viz as viz\n",
    "\n",
    "import importlib\n",
    "importlib.reload(viz)\n",
    "\n",
    "# Set custom plot style for consistency\n",
    "viz.set_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Data Injestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Datasets\n",
    "df_employee = catalog.load(\"employee_dataset\")\n",
    "df_salary = catalog.load(\"employee_salaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Employee Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Inspecting the employee dataset for basic information & statistics\n",
    "\n",
    "# Shape of the dataset\n",
    "print(f\"Dataset shape: {df_employee.shape}\")\n",
    "\n",
    "# 2. Preview first 5 rows\n",
    "display(df_employee.head())\n",
    "\n",
    "# 3. Column names and data types\n",
    "df_employee.info()\n",
    "\n",
    "# 4. Descriptive statistics for numerical and categorical features\n",
    "display(df_employee.describe(include='all'))\n",
    "\n",
    "# 5. Check for missing values\n",
    "missing_counts = df_employee.isnull().sum()\n",
    "missing_perc = (missing_counts / len(df_employee) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing Count': missing_counts, 'Missing %': missing_perc})\n",
    "display(missing_df[missing_df['Missing Count'] > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Initial issues or anomalies detected.\n",
    "\n",
    "- Column names are not in a standard format as they have upper & lower case\n",
    "- Data entries are unecessarily complex i.e. COMP37 as they could just be 37\n",
    "- Columns are not in the correct dtype\n",
    "- distanceFromCBD has a very large difference from 75% to MAX indicating high value outliers\n",
    "- missing data in multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Cleaning Employee Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial column names from data description\n",
    "# standardizing column names for consistency\n",
    "df_employee.rename(columns={\n",
    "    'jobId': 'job_id',\n",
    "    'companyId': 'company_id',\n",
    "    'jobRole': 'job_role',\n",
    "    'education': 'education',\n",
    "    'major': 'major',\n",
    "    'Industry': 'industry',\n",
    "    'yearsExperience': 'years_experience',\n",
    "    'distanceFromCBD': 'distance_from_cbd'\n",
    "}, inplace=True)\n",
    "\n",
    "df_employee.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Fixed column names to be more standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check value counts for company_id including NaN\n",
    "company_counts = df_employee['company_id'].value_counts(dropna=False)\n",
    "\n",
    "display(company_counts)\n",
    "print(f\"Unique company_id count (including NaN): {df_employee['company_id'].nunique(dropna=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "There is a good data spread in the company_id column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Checking the various unique entries as well as ensuring the prefix is COMP for all, as well as keeping a before prefix drop state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remove 'COMP' prefix and convert to integer\n",
    "df_employee['company_id'] = (\n",
    "    df_employee['company_id']\n",
    "    .astype(str)\n",
    "    .str.replace('COMP', '', regex=False)\n",
    "    .replace('<NA>', pd.NA)  # make sure string '<NA>' is real missing value\n",
    "    .astype('Int64')  # nullable integer dtype just for eda purposes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check value counts for company_id including NaN\n",
    "company_counts = df_employee['company_id'].value_counts(dropna=False)\n",
    "\n",
    "display(company_counts)\n",
    "print(f\"Unique company_id count (including NaN): {df_employee['company_id'].nunique(dropna=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Fixed company id column to be more model friendly whilst maintaining all entries including NA (to be dealt with when handling missing or dupe data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Next lets ensure the job_id column follows the same rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# pattern check for job_id\n",
    "pattern_check_job = df_employee['job_id'].apply(\n",
    "    lambda x: pd.isna(x) or bool(re.match(r'^JOB\\d+$', str(x)))\n",
    ")\n",
    "print(f\"All non-null job_id match 'JOBxxxx...' format?: {pattern_check_job.all()}\")\n",
    "\n",
    "# Remove 'JOB' prefix and convert to nullable integer\n",
    "df_employee['job_id'] = (\n",
    "    df_employee['job_id']\n",
    "    .astype(str)\n",
    "    .str.replace('JOB', '', regex=False)\n",
    "    .replace(['<NA>', 'nan', 'NaN'], pd.NA)  # ensure true missing values\n",
    "    .astype('Int64')  # nullable integer dtype\n",
    ")\n",
    "\n",
    "df_employee.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all string columns\n",
    "string_cols = df_employee.select_dtypes(include='string').columns\n",
    "\n",
    "# Display unique values for each string column\n",
    "for col in string_cols:\n",
    "    temp_series = df_employee[col].astype(str)  # ensure string format for display\n",
    "    \n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(f\"Unique values: {temp_series.nunique(dropna=False)}\")\n",
    "    print(temp_series.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "- job_role column seems to have a good spread of job roles except for a sole exception which is the president job role but all roles are valid and do not have semantic overlap. Some missing data but will be handled later.\n",
    "\n",
    "- education column has a large amount of missing data which is labeled as NONE and NA. Good spread of data without any semantic overlap.\n",
    "\n",
    "- major column has a similar issue with education column with missing data with 2 different labels but aside from missing data, there is a good spread of data among the categories with no semantic overlap.\n",
    "\n",
    "- industry column has a similar issue with job_role column. One singular entry in a category i.e. governement. However, there is a good spread of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We will treat NA and NONE as the same for those columns that have both appear at the same time. For the industry & job_role column which only have NA, as there is no clear category or large amount of missing rows, we will drop the entire row which has the missing data. For numeric columns as they only have <500 missing values, we will also just drop the entire row as they are not a significant portion of data as seen in initial inspection, this also includes the missing entries in job_id. This no tolerance for missing data will ensure the data is as complete as can be baring those columns which have a large portion of missing data which will be explicitly labeled as NONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns where NA/NaN should be treated as 'NONE'\n",
    "cols_na_to_none = ['education', 'major']\n",
    "\n",
    "for col in cols_na_to_none:\n",
    "    df_employee[col] = (\n",
    "        df_employee[col]\n",
    "        .replace(['NA', 'na', 'NaN', 'nan', '<NA>'], pd.NA)  # unify NA forms\n",
    "        .fillna('NONE')  # replace actual missing with 'NONE'\n",
    "    )\n",
    "\n",
    "# Preview changes\n",
    "for col in cols_na_to_none:\n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(df_employee[col].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in job_role, or industry\n",
    "cols_drop_na = ['job_role', 'industry']\n",
    "\n",
    "before_drop = len(df_employee)\n",
    "df_employee = df_employee.dropna(subset=cols_drop_na)\n",
    "after_drop = len(df_employee)\n",
    "\n",
    "print(f\"Dropped {before_drop - after_drop} rows due to NA in {cols_drop_na}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing data in integer columns\n",
    "int_cols = df_employee.select_dtypes(include=['int64', 'Int64']).columns\n",
    "\n",
    "before_drop = len(df_employee)\n",
    "df_employee = df_employee.dropna(subset=int_cols)\n",
    "after_drop = len(df_employee)\n",
    "\n",
    "print(f\"Dropped {before_drop - after_drop} rows due to NA in integer columns: {list(int_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for remaining missing values in all columns\n",
    "missing_counts = df_employee.isna().sum()\n",
    "missing_perc = (missing_counts / len(df_employee) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing %': missing_perc\n",
    "}).sort_values(by='Missing Count', ascending=False)\n",
    "\n",
    "display(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "This means there is no longer any column with NA entry they are either specifically mapped to NONE or dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Next lets fix the datatypes of the string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Check datatypes & memory usage of all columns\n",
    "df_employee.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "- Since we no longer have any nulls, nullable ints are no longer needed. In data preparation pipeline, we will ensure that missing data is dropped immediately and not converted to Int64 and then dropped later which would be less memory efficient (Done here to help with flow of EDA)\n",
    "\n",
    "- To ensure memory usage is minimal, we will convert string to categorical dtype\n",
    "\n",
    "- One specific case will be company_id as it is nominal and not ordinal, we will convert it to a categorical as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert company_id to category\n",
    "if 'company_id' in df_employee.columns:\n",
    "    df_employee['company_id'] = df_employee['company_id'].astype('category')\n",
    "\n",
    "# Convert string columns to category for efficient EDA\n",
    "for col in df_employee.select_dtypes(include='string').columns:\n",
    "    df_employee[col] = df_employee[col].astype('category')\n",
    "\n",
    "# Convert nullable Int64 columns to regular int64\n",
    "int_cols_nullable = df_employee.select_dtypes(include='Int64').columns\n",
    "df_employee[int_cols_nullable] = df_employee[int_cols_nullable].astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Check datatypes & memory usage of all columns\n",
    "df_employee.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "we can see a reduction from 72.5 MB to 35.3 MB. This is a good optimization method. Especially considering that in big data reducing the memory usage of the data will help with speed of all downstream tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Done for now with cleaning of Employee dataset. Analysis of data with graphs will be done after cleaning of salary dataset & merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Salary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Inspecting the salary dataset for basic information & statistics\n",
    "\n",
    "# Shape of the dataset\n",
    "print(f\"Dataset shape: {df_salary.shape}\")\n",
    "\n",
    "# 2. Preview first 5 rows\n",
    "display(df_salary.head())\n",
    "\n",
    "# 3. Column names and data types\n",
    "df_salary.info()\n",
    "\n",
    "# 4. Descriptive statistics for numerical and categorical features\n",
    "display(df_salary.describe(include='all'))\n",
    "\n",
    "# 5. Check for missing values\n",
    "missing_counts = df_salary.isnull().sum()\n",
    "missing_perc = (missing_counts / len(df_salary) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing Count': missing_counts, 'Missing %': missing_perc})\n",
    "display(missing_df[missing_df['Missing Count'] > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Issues or anomalies with dataset\n",
    "\n",
    "- non standardized column names\n",
    "- complex job_id data entry\n",
    "- missing data\n",
    "- improper dtypes\n",
    "- Clear high value outlier in salaryInThousands column\n",
    "- min value is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Cleaning Salary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_salary.rename(columns={\n",
    "    'jobId': 'job_id',\n",
    "    'salaryInThousands': 'salary_k'\n",
    "}, inplace=True)\n",
    "\n",
    "# Preview to confirm\n",
    "df_salary.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Clean job_id (handle missing + strip JOB)\n",
    "df_salary['job_id'] = (\n",
    "    df_salary['job_id']\n",
    "    .astype('string')                         # work in string mode for replace\n",
    "    .str.replace('JOB', '', regex=False)      # remove 'JOB' prefix\n",
    "    .astype('Int64')                          # nullable integer dtype\n",
    ")\n",
    "\n",
    "# Preview to confirm\n",
    "df_salary[['job_id']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing job_id or salary_k\n",
    "before_drop = len(df_salary)\n",
    "df_salary = df_salary.dropna(subset=['job_id', 'salary_k'])\n",
    "after_drop = len(df_salary)\n",
    "\n",
    "print(f\"Dropped {before_drop - after_drop} rows with missing job_id or salary_k\")\n",
    "print(f\"Remaining rows: {after_drop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking column dtype & memory usage\n",
    "df_salary.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to non-nullable int64\n",
    "df_salary['job_id'] = df_salary['job_id'].astype('int64')\n",
    "df_salary['salary_k'] = df_salary['salary_k'].astype('int64')\n",
    "\n",
    "print(df_salary.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking column dtype & memory usage\n",
    "df_salary.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Merged Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Merging both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare job_id counts and matches\n",
    "emp_ids = set(df_employee['job_id'])\n",
    "sal_ids = set(df_salary['job_id'])\n",
    "\n",
    "print(f\"Unique job_ids in employee dataset: {len(emp_ids)}\")\n",
    "print(f\"Unique job_ids in salary dataset:   {len(sal_ids)}\")\n",
    "\n",
    "# Check exact match\n",
    "print(f\"All employee job_ids in salary dataset? {emp_ids.issubset(sal_ids)}\")\n",
    "print(f\"All salary job_ids in employee dataset? {sal_ids.issubset(emp_ids)}\")\n",
    "\n",
    "# Find differences\n",
    "missing_in_salary = emp_ids - sal_ids\n",
    "missing_in_employee = sal_ids - emp_ids\n",
    "\n",
    "print(f\"Job_ids in employee but not in salary: {len(missing_in_salary)}\")\n",
    "print(f\"Job_ids in salary but not in employee: {len(missing_in_employee)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "Since there are missing ID's in both datasets we will use an inner join to ensure only ids that are present make it into the final merged dataset to ensure no new null values are introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track before merge\n",
    "rows_emp_before = len(df_employee)\n",
    "rows_sal_before = len(df_salary)\n",
    "\n",
    "# Inner join\n",
    "df_merged = df_employee.merge(df_salary, on='job_id', how='inner')\n",
    "\n",
    "# Track after merge\n",
    "rows_after = len(df_merged)\n",
    "\n",
    "print(f\"Rows in employee dataset before merge: {rows_emp_before}\")\n",
    "print(f\"Rows in salary dataset before merge:   {rows_sal_before}\")\n",
    "print(f\"Rows after inner join:                  {rows_after}\")\n",
    "\n",
    "print(f\"Rows lost from employee dataset: {rows_emp_before - rows_after}\")\n",
    "print(f\"Rows lost from salary dataset:   {rows_sal_before - rows_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the final merged dataset to ensure inner join worked as expected\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Inner join went well and minimal data was lost from the initial 1,000,000 to 999479. Now we will move onto data cleaning on merged dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Cleaning Merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Inspecting the merged dataset for basic information & statistics\n",
    "\n",
    "# Shape of the dataset\n",
    "print(f\"Dataset shape: {df_merged.shape}\")\n",
    "\n",
    "# 2. Preview first 5 rows\n",
    "display(df_merged.head())\n",
    "\n",
    "# Descriptive statistics (excluding job_id but keeping salary_k) with thousands separator\n",
    "display(df_merged.drop(columns=['job_id']).describe(include='all').style.format(thousands=',', precision=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "The main thing needed to be done now is to handle outliers in the numerical & categorical columns. There is also a need to check for complete duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for full duplicate rows\n",
    "full_dupes = df_merged.duplicated().sum()\n",
    "print(f\"Full duplicate rows: {full_dupes}\")\n",
    "\n",
    "# Check for duplicate job_id values\n",
    "jobid_dupes = df_merged['job_id'].duplicated().sum()\n",
    "print(f\"Duplicate job_id count: {jobid_dupes}\")\n",
    "\n",
    "# Display the first few duplicate job_id entries if present\n",
    "if jobid_dupes > 0:\n",
    "    display(df_merged[df_merged['job_id'].duplicated(keep=False)].sort_values('job_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "Lets go column by column when handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_categoricals(df=df_merged,cols=['job_role'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "From the graph, we can once again confirm, that there is a good spread of data however there is a clear singular outlier. Unfortunately, as there is no significant count of president category job_role, this means that this singular president may skew the averages of one specific industry or company or job role as he/she is not representative of the majority thus affecting eda later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the president outlier from job_role\n",
    "df_merged = df_merged[df_merged['job_role'] != 'PRESIDENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'PRESIDENT' from job_role if it exists\n",
    "df_merged['job_role'] = df_merged['job_role'].cat.remove_unused_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_categoricals(df=df_merged,cols=['education'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "This column has no major outliers, however there is a large number of NONE present however as they are explicitly labled it will not be an issue later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_categoricals(df=df_merged,cols=['major'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "This column has no major outliers and spread of data is actually good, however there is a large number of NONE present however as they are explicitly labled it will not be an issue later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_count = df_merged['education'].isin(['HIGH_SCHOOL', 'NONE']).sum()\n",
    "print(f\"Combined count for HIGH_SCHOOL & NONE: {combined_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "There seems to be a number of missing values from the major column that are true missing values and cannot be explained by missing or semantic information from another column. This however is not an issue as we are already assuming all types of missing data to be the same and then handled as NONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_categoricals(df=df_merged,cols=['industry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "This is quite unexpected. It seems the one entry we removed i.e the president row was actually the president of the country as it seems the Government column is no longer in use. Meaning that our removal of the one row was all the more valid as a country only has one president therefore he/she would not have been at all representative of the majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'GOVERNMENT' from job_role if it exists\n",
    "df_merged['industry'] = df_merged['industry'].cat.remove_unused_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for numeric distribution plot\n",
    "df_numeric = df_merged[['years_experience', 'distance_from_cbd']]\n",
    "viz.plot_numeric_distribution(df=df_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "- years_experience column has good data spread as the boxes & whiskers are of equal length. It also has no outliers as there are no markers.\n",
    "\n",
    "- distance_from_cbd column has generally good data spread as the whiskers and boxes are of even length. However, there are 2 high value outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting distance_from_cbd outliers\n",
    "df_merged[df_merged['distance_from_cbd'] > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "- It seems there are only 2 of these instances and are too far from 75th quanitile + 1.5 x IQR as such we will just drop these 2 rows to ensure simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers from distance_from_cbd\n",
    "df_merged = df_merged[df_merged['distance_from_cbd'] <= 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "We will deal with salary_k next seperately. This is because there are very large value outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of salary_k\n",
    "df_merged['salary_k'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting salary_k == 0\n",
    "df_merged[df_merged['salary_k'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "It clearly seems that these values were wrongly keyed in, as these people all have jobs and some even have multiple years of experience. Therefore, these rows of data are clearly invalid entries. These will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping salary_k == 0 rows\n",
    "df_merged = df_merged[df_merged['salary_k'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of salary_k\n",
    "df_merged['salary_k'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "It also seems like there is a input of 10 million as well. Let's review the top 25 highest earners manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view top 25 salary_k\n",
    "top_25_salaries = df_merged.nlargest(25, 'salary_k')\n",
    "top_25_salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "Clearly, we can see that the 10000000k entry is incorrect as its the only one of its kind and needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 10000000k entry\n",
    "df_merged = df_merged[df_merged['salary_k'] != 10000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of salary_k\n",
    "df_merged['salary_k'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "This has drastically reduced the range of values to normal human levels. Since we already reviewed the other 24 highest earners and they were genuine outliers, we cannot just haphazardly drop them. Therefore, lets verify the lowest 25 are also genuine outliers and if so let us log transform the values to reduce the skew of the data if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing bottom 25 salary_k\n",
    "bottom_25_salaries = df_merged.nsmallest(25, 'salary_k')\n",
    "bottom_25_salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "Yes this has confirmed my findings, all remaining salary_k values are valid and contain genuine outliers. Therefore, removing them is not required as they represent a portion of the job market. Lets, calculate the skew to verify if log transform is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % df_merged['salary_k'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "The spread of the data is acceptable and is within the range of skewness for a fairly symmetrical bell curve distribution of between -0.5 to 0.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "## What is the highest paying job for the web industry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "First lets filter out dataset to only include the web industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_web = df_merged[df_merged[\"industry\"] == \"WEB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "Next I will group the respective jobs together and take the median value of them, we do this to get the median pay of each job. This is to ensure the average is not skewed by outliers as mean often times causes skewed data. We will also review the sample size of each job role to ensure each job role has sufficient representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by job_role in Web industry\n",
    "web_roles_stats = (df_web.groupby('job_role', observed=True)['salary_k'].agg(median_salary='median', count='size').sort_values(by='median_salary', ascending=False))\n",
    "\n",
    "display(web_roles_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "Here we can see that the sample size for each job role is similar therefore we can compare the jobs without any issues. Lets plot the table to a bar chart to view the data with clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already grouped and calculated medians separately\n",
    "viz.plot_bar(df=web_roles_stats.reset_index(), x_col='job_role', y_col='median_salary',title=\"Median Salary by Job Role in Web Industry\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "From the bar chart, we can clearly see that the job role with the highest mediaan pay in the web industry is CEO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "##  Rank the top 10 jobs roles with the highest salary for all the industry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "Since this time we do not need to analyse a specific industry, we can procceed to the group by step immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by industry + job_role and compute median salary + counts\n",
    "role_salary_stats = (\n",
    "    df_merged.groupby(['industry', 'job_role'], observed=True)['salary_k']\n",
    "    .agg(median_salary='median', count='size')\n",
    "    .reset_index()\n",
    "    .sort_values(by='median_salary', ascending=False)\n",
    ")\n",
    "\n",
    "# Select the top 10 roles by median salary across all industries\n",
    "top10_roles_by_industry = role_salary_stats.head(10).copy()\n",
    "\n",
    "# Create new combined column\n",
    "top10_roles_by_industry['role_label'] = (\n",
    "    top10_roles_by_industry['industry'].astype(str) \n",
    "    + ' - ' + \n",
    "    top10_roles_by_industry['job_role'].astype(str)\n",
    ")\n",
    "\n",
    "# Drop the original two columns\n",
    "top10_roles_by_industry = top10_roles_by_industry.drop(columns=['industry', 'job_role'])\n",
    "\n",
    "# Reorder columns so role_label is first\n",
    "cols = ['role_label'] + [c for c in top10_roles_by_industry.columns if c != 'role_label']\n",
    "\n",
    "top10_roles_by_industry = top10_roles_by_industry[cols]\n",
    "\n",
    "top10_roles_by_industry.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(top10_roles_by_industry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "We can see that sample size of each job role is represented well, therefore we can proceed with the plotting of the df to make our final observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bar(df=top10_roles_by_industry.reset_index(), x_col='role_label', y_col='median_salary',title=\"Top 10 Job Roles across all industries by Median Salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "These are the top 10 jobs ranked by median salary across all industries with the highest paid job roles on the left and the lowest on the right side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "## Which of the industries has the highest salary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "This time since we are only comparing across industries which contains a range of jobs, we need to have a robust method to compare the average salary of each industry. So what I will do is compare is highest lowest and median salary across all industries to get a more overarching view of the salaries of each type of individual i.e. median -> Typical employee, lowest -> lowest paying job-role & the highest -> best paying job-role in the industry. This should help the government understand which industries allow for growth of the employees and which industries have stagnant pay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each industry, get lowest, median, and highest job-role medians\n",
    "industry_salary_profile = (\n",
    "    df_merged.groupby('industry', observed=True)['salary_k']\n",
    "    .agg(count='size', lowest_salary='min', median_salary='median', highest_salary='max')\n",
    "    .reset_index()\n",
    "    .sort_values(by='median_salary', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate delta (growth potential)\n",
    "industry_salary_profile['salary_delta'] = industry_salary_profile['highest_salary'] - industry_salary_profile['lowest_salary']\n",
    "industry_salary_profile.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the industry salary profile\n",
    "display(industry_salary_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "Here we can see that the sample size for each of the industries are very similar therefore we can compare without any additional steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_clustered_bars(\n",
    "    df=industry_salary_profile,\n",
    "    x_col='industry',\n",
    "    y_cols=['median_salary', 'highest_salary', 'lowest_salary','salary_delta'],\n",
    "    title=\"Median Salary, Highest Salary & Salary Delta by Industry\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "This clustered bar chart tells us quite a lot of information. We can observe that the typical employee in the Oil & Finance industry get paid the most at 128k. However, to differenciate between the two industries to find the \"highest\" paying we can then compare the highest pay, lowest pay & salary delta. Between the two industries, the oil industry has the highest salary at 301k as compared to the finance industries highest salary of 294k. Furthermore, when comparing the lowest salary, we can see that there is only a small difference of 1k but the oil industry is better paying at 37k as compared to the finance industry's 36k. When comparing the salary delta (Potential for growth) we can see that thee oil industry comes out ahead again at 264k compared to the finance industries 258k. Therefore, in conclusion typically for most employees, either the finance or oil industry will lead to the highest pay. However, if selecting the highest paying industry, the oil industry is the best paying industry when comparing all different levels of pay as well as the potential for income growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {},
   "source": [
    "## Which job has the lowest pay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
