{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Project Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kedro \n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set Kedro project path\n",
    "project_path = Path.cwd().parent\n",
    "\n",
    "# Bootstrap Kedro\n",
    "from kedro.framework.startup import bootstrap_project\n",
    "from kedro.framework.session import KedroSession\n",
    "\n",
    "bootstrap_project(project_path)\n",
    "session = KedroSession.create(project_path=project_path)\n",
    "context = session.load_context()\n",
    "catalog = context.catalog\n",
    "\n",
    "# Add src/ to Python path\n",
    "sys.path.append(str(project_path / \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) # Ignore FutureWarnings globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import full modules (for reload)\n",
    "import egt305_job_market_analysis.utils.viz as viz\n",
    "\n",
    "import importlib\n",
    "importlib.reload(viz)\n",
    "\n",
    "# Set custom plot style for consistency\n",
    "viz.set_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Data Injestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Datasets\n",
    "df_employee = catalog.load(\"employee_dataset_raw\")\n",
    "df_salary = catalog.load(\"employee_salaries_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Employee Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Inspecting the employee dataset for basic information & statistics\n",
    "\n",
    "# Shape of the dataset\n",
    "print(f\"Dataset shape: {df_employee.shape}\")\n",
    "\n",
    "# 2. Preview first 5 rows\n",
    "display(df_employee.head())\n",
    "\n",
    "# 3. Column names and data types\n",
    "df_employee.info()\n",
    "\n",
    "# 4. Descriptive statistics for numerical and categorical features\n",
    "display(df_employee.describe(include='all'))\n",
    "\n",
    "# 5. Check for missing values\n",
    "missing_counts = df_employee.isnull().sum()\n",
    "missing_perc = (missing_counts / len(df_employee) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing Count': missing_counts, 'Missing %': missing_perc})\n",
    "display(missing_df[missing_df['Missing Count'] > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Initial issues or anomalies detected.\n",
    "\n",
    "- Column names are not in a standard format as they have upper & lower case\n",
    "- Data entries are unecessarily complex i.e. COMP37 as they could just be 37\n",
    "- Columns are not in the correct dtype\n",
    "- distanceFromCBD has a very large difference from 75% to MAX indicating high value outliers\n",
    "- missing data in multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Cleaning Employee Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial column names from data description\n",
    "# standardizing column names for consistency\n",
    "df_employee.rename(columns={\n",
    "    'jobId': 'job_id',\n",
    "    'companyId': 'company_id',\n",
    "    'jobRole': 'job_role',\n",
    "    'education': 'education',\n",
    "    'major': 'major',\n",
    "    'Industry': 'industry',\n",
    "    'yearsExperience': 'years_experience',\n",
    "    'distanceFromCBD': 'distance_from_cbd'\n",
    "}, inplace=True)\n",
    "\n",
    "df_employee.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Fixed column names to be more standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check value counts for company_id including NaN\n",
    "company_counts = df_employee['company_id'].value_counts(dropna=False)\n",
    "\n",
    "display(company_counts)\n",
    "print(f\"Unique company_id count (including NaN): {df_employee['company_id'].nunique(dropna=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "There is a good data spread in the company_id column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Checking the various unique entries as well as ensuring the prefix is COMP for all, as well as keeping a before prefix drop state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remove 'COMP' prefix and convert to integer\n",
    "df_employee['company_id'] = (\n",
    "    df_employee['company_id']\n",
    "    .astype(str)\n",
    "    .str.replace('COMP', '', regex=False)\n",
    "    .replace('<NA>', pd.NA)  # make sure string '<NA>' is real missing value\n",
    "    .astype('Int64')  # nullable integer dtype just for eda purposes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check value counts for company_id including NaN\n",
    "company_counts = df_employee['company_id'].value_counts(dropna=False)\n",
    "\n",
    "display(company_counts)\n",
    "print(f\"Unique company_id count (including NaN): {df_employee['company_id'].nunique(dropna=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Fixed company id column to be more model friendly whilst maintaining all entries including NA (to be dealt with when handling missing or dupe data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Next lets ensure the job_id column follows the same rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# pattern check for job_id\n",
    "pattern_check_job = df_employee['job_id'].apply(\n",
    "    lambda x: pd.isna(x) or bool(re.match(r'^JOB\\d+$', str(x)))\n",
    ")\n",
    "print(f\"All non-null job_id match 'JOBxxxx...' format?: {pattern_check_job.all()}\")\n",
    "\n",
    "# Remove 'JOB' prefix and convert to nullable integer\n",
    "df_employee['job_id'] = (\n",
    "    df_employee['job_id']\n",
    "    .astype(str)\n",
    "    .str.replace('JOB', '', regex=False)\n",
    "    .replace(['<NA>', 'nan', 'NaN'], pd.NA)  # ensure true missing values\n",
    "    .astype('Int64')  # nullable integer dtype\n",
    ")\n",
    "\n",
    "df_employee.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all string columns\n",
    "string_cols = df_employee.select_dtypes(include='string').columns\n",
    "\n",
    "# Display unique values for each string column\n",
    "for col in string_cols:\n",
    "    temp_series = df_employee[col].astype(str)  # ensure string format for display\n",
    "    \n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(f\"Unique values: {temp_series.nunique(dropna=False)}\")\n",
    "    print(temp_series.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "- job_role column seems to have a good spread of job roles except for a sole exception which is the president job role but all roles are valid and do not have semantic overlap. Some missing data but will be handled later.\n",
    "\n",
    "- education column has a large amount of missing data which is labeled as NONE and NA. Good spread of data without any semantic overlap.\n",
    "\n",
    "- major column has a similar issue with education column with missing data with 2 different labels but aside from missing data, there is a good spread of data among the categories with no semantic overlap.\n",
    "\n",
    "- industry column has a similar issue with job_role column. One singular entry in a category i.e. governement. However, there is a good spread of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We will treat NA and NONE as the same for those columns that have both appear at the same time. For the industry & job_role column which only have NA, as there is no clear category or large amount of missing rows, we will drop the entire row which has the missing data. For numeric columns as they only have <500 missing values, we will also just drop the entire row as they are not a significant portion of data as seen in initial inspection, this also includes the missing entries in job_id. This no tolerance for missing data will ensure the data is as complete as can be baring those columns which have a large portion of missing data which will be explicitly labeled as NONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns where NA/NaN should be treated as 'NONE'\n",
    "cols_na_to_none = ['education', 'major']\n",
    "\n",
    "for col in cols_na_to_none:\n",
    "    df_employee[col] = (\n",
    "        df_employee[col]\n",
    "        .replace(['NA', 'na', 'NaN', 'nan', '<NA>'], pd.NA)  # unify NA forms\n",
    "        .fillna('NONE')  # replace actual missing with 'NONE'\n",
    "    )\n",
    "\n",
    "# Preview changes\n",
    "for col in cols_na_to_none:\n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(df_employee[col].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in job_role, or industry\n",
    "cols_drop_na = ['job_role', 'industry']\n",
    "\n",
    "before_drop = len(df_employee)\n",
    "df_employee = df_employee.dropna(subset=cols_drop_na)\n",
    "after_drop = len(df_employee)\n",
    "\n",
    "print(f\"Dropped {before_drop - after_drop} rows due to NA in {cols_drop_na}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing data in integer columns\n",
    "int_cols = df_employee.select_dtypes(include=['int64', 'Int64']).columns\n",
    "\n",
    "before_drop = len(df_employee)\n",
    "df_employee = df_employee.dropna(subset=int_cols)\n",
    "after_drop = len(df_employee)\n",
    "\n",
    "print(f\"Dropped {before_drop - after_drop} rows due to NA in integer columns: {list(int_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for remaining missing values in all columns\n",
    "missing_counts = df_employee.isna().sum()\n",
    "missing_perc = (missing_counts / len(df_employee) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing %': missing_perc\n",
    "}).sort_values(by='Missing Count', ascending=False)\n",
    "\n",
    "display(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "This means there is no longer any column with NA entry they are either specifically mapped to NONE or dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Next lets fix the datatypes of the string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Check datatypes & memory usage of all columns\n",
    "df_employee.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "- Since we no longer have any nulls, nullable ints are no longer needed. In data preparation pipeline, we will ensure that missing data is dropped immediately and not converted to Int64 and then dropped later which would be less memory efficient (Done here to help with flow of EDA)\n",
    "\n",
    "- To ensure memory usage is minimal, we will convert string to categorical dtype\n",
    "\n",
    "- One specific case will be company_id as it is nominal and not ordinal, we will convert it to a categorical as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert company_id to category\n",
    "if 'company_id' in df_employee.columns:\n",
    "    df_employee['company_id'] = df_employee['company_id'].astype('category')\n",
    "\n",
    "# Convert string columns to category for efficient EDA\n",
    "for col in df_employee.select_dtypes(include='string').columns:\n",
    "    df_employee[col] = df_employee[col].astype('category')\n",
    "\n",
    "# Convert nullable Int64 columns to regular int64\n",
    "int_cols_nullable = df_employee.select_dtypes(include='Int64').columns\n",
    "df_employee[int_cols_nullable] = df_employee[int_cols_nullable].astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Check datatypes & memory usage of all columns\n",
    "df_employee.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "we can see a reduction from 72.5 MB to 35.3 MB. This is a good optimization method. Especially considering that in big data reducing the memory usage of the data will help with speed of all downstream tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Done for now with cleaning of Employee dataset. Analysis of data with graphs will be done after cleaning of salary dataset & merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Salary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Inspecting the salary dataset for basic information & statistics\n",
    "\n",
    "# Shape of the dataset\n",
    "print(f\"Dataset shape: {df_salary.shape}\")\n",
    "\n",
    "# 2. Preview first 5 rows\n",
    "display(df_salary.head())\n",
    "\n",
    "# 3. Column names and data types\n",
    "df_salary.info()\n",
    "\n",
    "# 4. Descriptive statistics for numerical and categorical features\n",
    "display(df_salary.describe(include='all'))\n",
    "\n",
    "# 5. Check for missing values\n",
    "missing_counts = df_salary.isnull().sum()\n",
    "missing_perc = (missing_counts / len(df_salary) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing Count': missing_counts, 'Missing %': missing_perc})\n",
    "display(missing_df[missing_df['Missing Count'] > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Issues or anomalies with dataset\n",
    "\n",
    "- non standardized column names\n",
    "- complex job_id data entry\n",
    "- missing data\n",
    "- improper dtypes\n",
    "- Clear high value outlier in salaryInThousands column\n",
    "- min value is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Cleaning Salary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_salary.rename(columns={\n",
    "    'jobId': 'job_id',\n",
    "    'salaryInThousands': 'salary_k'\n",
    "}, inplace=True)\n",
    "\n",
    "# Preview to confirm\n",
    "df_salary.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Clean job_id (handle missing + strip JOB)\n",
    "df_salary['job_id'] = (\n",
    "    df_salary['job_id']\n",
    "    .astype('string')                         # work in string mode for replace\n",
    "    .str.replace('JOB', '', regex=False)      # remove 'JOB' prefix\n",
    "    .astype('Int64')                          # nullable integer dtype\n",
    ")\n",
    "\n",
    "# Preview to confirm\n",
    "df_salary[['job_id']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing job_id or salary_k\n",
    "before_drop = len(df_salary)\n",
    "df_salary = df_salary.dropna(subset=['job_id', 'salary_k'])\n",
    "after_drop = len(df_salary)\n",
    "\n",
    "print(f\"Dropped {before_drop - after_drop} rows with missing job_id or salary_k\")\n",
    "print(f\"Remaining rows: {after_drop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking column dtype & memory usage\n",
    "df_salary.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to non-nullable int64\n",
    "df_salary['job_id'] = df_salary['job_id'].astype('int64')\n",
    "df_salary['salary_k'] = df_salary['salary_k'].astype('int64')\n",
    "\n",
    "print(df_salary.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking column dtype & memory usage\n",
    "df_salary.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Merged Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Merging both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare job_id counts and matches\n",
    "emp_ids = set(df_employee['job_id'])\n",
    "sal_ids = set(df_salary['job_id'])\n",
    "\n",
    "print(f\"Unique job_ids in employee dataset: {len(emp_ids)}\")\n",
    "print(f\"Unique job_ids in salary dataset:   {len(sal_ids)}\")\n",
    "\n",
    "# Check exact match\n",
    "print(f\"All employee job_ids in salary dataset? {emp_ids.issubset(sal_ids)}\")\n",
    "print(f\"All salary job_ids in employee dataset? {sal_ids.issubset(emp_ids)}\")\n",
    "\n",
    "# Find differences\n",
    "missing_in_salary = emp_ids - sal_ids\n",
    "missing_in_employee = sal_ids - emp_ids\n",
    "\n",
    "print(f\"Job_ids in employee but not in salary: {len(missing_in_salary)}\")\n",
    "print(f\"Job_ids in salary but not in employee: {len(missing_in_employee)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "Since there are missing ID's in both datasets we will use an inner join to ensure only ids that are present make it into the final merged dataset to ensure no new null values are introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track before merge\n",
    "rows_emp_before = len(df_employee)\n",
    "rows_sal_before = len(df_salary)\n",
    "\n",
    "# Inner join\n",
    "df_merged = df_employee.merge(df_salary, on='job_id', how='inner')\n",
    "\n",
    "# Track after merge\n",
    "rows_after = len(df_merged)\n",
    "\n",
    "print(f\"Rows in employee dataset before merge: {rows_emp_before}\")\n",
    "print(f\"Rows in salary dataset before merge:   {rows_sal_before}\")\n",
    "print(f\"Rows after inner join:                  {rows_after}\")\n",
    "\n",
    "print(f\"Rows lost from employee dataset: {rows_emp_before - rows_after}\")\n",
    "print(f\"Rows lost from salary dataset:   {rows_sal_before - rows_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the final merged dataset to ensure inner join worked as expected\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Inner join went well and minimal data was lost from the initial 1,000,000 to 999479. Now we will move onto data cleaning on merged dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Cleaning Merged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "#### Inspecting Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Inspecting the merged dataset for basic information & statistics\n",
    "\n",
    "# Shape of the dataset\n",
    "print(f\"Dataset shape: {df_merged.shape}\")\n",
    "\n",
    "# 2. Preview first 5 rows\n",
    "display(df_merged.head())\n",
    "\n",
    "# Descriptive statistics (excluding job_id but keeping salary_k) with thousands separator\n",
    "display(df_merged.drop(columns=['job_id']).describe(include='all').style.format(thousands=',', precision=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "The main thing needed to be done now is to handle outliers in the numerical & categorical columns. There is also a need to check for complete duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "#### Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for full duplicate rows\n",
    "full_dupes = df_merged.duplicated().sum()\n",
    "print(f\"Full duplicate rows: {full_dupes}\")\n",
    "\n",
    "# Check for duplicate job_id values\n",
    "jobid_dupes = df_merged['job_id'].duplicated().sum()\n",
    "print(f\"Duplicate job_id count: {jobid_dupes}\")\n",
    "\n",
    "# Display the first few duplicate job_id entries if present\n",
    "if jobid_dupes > 0:\n",
    "    display(df_merged[df_merged['job_id'].duplicated(keep=False)].sort_values('job_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "#### Handling invalid outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "Lets go column by column when handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_categoricals(df=df_merged,cols=['job_role'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "From the graph, we can once again confirm, that there is a good spread of data however there is a clear singular outlier. Unfortunately, as there is no significant count of president category job_role, this means that this singular president may skew the averages of one specific industry or company or job role as he/she is not representative of the majority thus affecting eda later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the president outlier from job_role\n",
    "df_merged = df_merged[df_merged['job_role'] != 'PRESIDENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'PRESIDENT' from job_role if it exists\n",
    "df_merged['job_role'] = df_merged['job_role'].cat.remove_unused_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_categoricals(df=df_merged,cols=['education'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "This column has no major outliers, however there is a large number of NONE present however as they are explicitly labled it will not be an issue later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_categoricals(df=df_merged,cols=['major'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "This column has no major outliers and spread of data is actually good, however there is a large number of NONE present however as they are explicitly labled it will not be an issue later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_count = df_merged['education'].isin(['HIGH_SCHOOL', 'NONE']).sum()\n",
    "print(f\"Combined count for HIGH_SCHOOL & NONE: {combined_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "There seems to be a number of missing values from the major column that are true missing values and cannot be explained by missing or semantic information from another column. This however is not an issue as we are already assuming all types of missing data to be the same and then handled as NONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_categoricals(df=df_merged,cols=['industry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "This is quite unexpected. It seems the one entry we removed i.e the president row was actually the president of the country as it seems the Government column is no longer in use. Meaning that our removal of the one row was all the more valid as a country only has one president therefore he/she would not have been at all representative of the majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'GOVERNMENT' from job_role if it exists\n",
    "df_merged['industry'] = df_merged['industry'].cat.remove_unused_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for numeric distribution plot\n",
    "df_numeric = df_merged[['years_experience', 'distance_from_cbd']]\n",
    "viz.plot_numeric_distribution(df=df_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "- years_experience column has good data spread as the boxes & whiskers are of equal length. It also has no outliers as there are no markers.\n",
    "\n",
    "- distance_from_cbd column has generally good data spread as the whiskers and boxes are of even length. However, there are 2 high value outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting distance_from_cbd outliers\n",
    "df_merged[df_merged['distance_from_cbd'] > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "- It seems there are only 2 of these instances and are too far from 75th quanitile + 1.5 x IQR as such we will just drop these 2 rows to ensure simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers from distance_from_cbd\n",
    "df_merged = df_merged[df_merged['distance_from_cbd'] <= 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "We will deal with salary_k next seperately. This is because there are very large value outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of salary_k\n",
    "df_merged['salary_k'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting salary_k == 0\n",
    "df_merged[df_merged['salary_k'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "It clearly seems that these values were wrongly keyed in, as these people all have jobs and some even have multiple years of experience. Therefore, these rows of data are clearly invalid entries. These will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping salary_k == 0 rows\n",
    "df_merged = df_merged[df_merged['salary_k'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of salary_k\n",
    "df_merged['salary_k'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "It also seems like there is a input of 10 million as well. Let's review the top 25 highest earners manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view top 25 salary_k\n",
    "top_25_salaries = df_merged.nlargest(25, 'salary_k')\n",
    "top_25_salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "Clearly, we can see that the 10000000k entry is incorrect as its the only one of its kind and needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 10000000k entry\n",
    "df_merged = df_merged[df_merged['salary_k'] != 10000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of salary_k\n",
    "df_merged['salary_k'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "This has drastically reduced the range of values to normal human levels. Since we already reviewed the other 24 highest earners and they were genuine outliers, we cannot just haphazardly drop them. Therefore, lets verify the lowest 25 are also genuine outliers and if so let us log transform the values to reduce the skew of the data if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing bottom 25 salary_k\n",
    "bottom_25_salaries = df_merged.nsmallest(25, 'salary_k')\n",
    "bottom_25_salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "Yes this has confirmed my findings, all remaining salary_k values are valid and contain genuine outliers. Therefore, removing them is not required as they represent a portion of the job market. Lets, calculate the skew to verify if log transform is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % df_merged['salary_k'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "The spread of the data is acceptable and is within the range of skewness for a fairly symmetrical bell curve distribution of between -0.5 to 0.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "#### Analysing improbable situations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "Next now that the invalid entries have been removed and no longer affect the spread of data, we can now finally proceed to capping the extreme outliers this is to ensure that the final analysis is done on mostly generic data. We will also log how many rows we dropped before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobrole_salary_profile = (\n",
    "    df_merged.groupby('job_role', observed=True)['salary_k']\n",
    "    .agg(count='size',\n",
    "         lowest_salary='min',\n",
    "         median_salary='median',\n",
    "         highest_salary='max')\n",
    "    .reset_index()\n",
    "    .sort_values(by='median_salary', ascending=True)  # sort ascending for lowest pay\n",
    ")\n",
    "# Display the job role salary profile\n",
    "display(jobrole_salary_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "There is still quite a large range of values. One issue that stands out to me is the janitor job role. This is because other positions may have a possible scenario where the person may get paid that salary depending on what they do or the position. However, after some research to gain insight, I have come to the conclusion that the janitor role is very unlikely to have a salary of 189k even if they were a custodian engineer which is a type of janitor that is paid maximum ~145k lets plot some graphs to visualise this data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x=\"job_role\", y=\"salary_k\", data=df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "Upon further inspection, the Janitor job role is displaying suspicious salary values. As aforementioned, external research indicates that the highest-paid custodial roles (e.g., custodian engineers) earn around $149k. However, our dataset included Janitors earning salaries exceeding those of the median salary of CEOs, CFOs, and CTOs, which is highly unlikely. To address this, I will apply the well-established IQR rule (Q1 – 1.5×IQR, Q3 + 1.5×IQR) to the Janitor role only. This ensures we remove unrealistic outliers while preserving valid executive-level salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Janitors and compute IQR for salary_k\n",
    "janitors = df_merged[df_merged['job_role'] == \"JANITOR\"].copy()\n",
    "\n",
    "# Compute Q1, Q3, and IQR for Janitors\n",
    "Q1 = janitors['salary_k'].quantile(0.25)\n",
    "Q3 = janitors['salary_k'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_limit = Q1 - 1.5 * IQR\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "print(\"Janitor Salary IQR Limits:\")\n",
    "print(\"Q1:\", Q1)\n",
    "print(\"Q3:\", Q3)\n",
    "print(\"Lower limit:\", lower_limit)\n",
    "print(\"Upper limit:\", upper_limit)\n",
    "\n",
    "# Apply the filter: keep only Janitors within bounds\n",
    "janitors_clean = janitors[\n",
    "    (janitors['salary_k'] >= lower_limit) &\n",
    "    (janitors['salary_k'] <= upper_limit)\n",
    "]\n",
    "\n",
    "# For other roles, keep everything as-is\n",
    "non_janitors = df_merged[df_merged['job_role'] != \"JANITOR\"]\n",
    "\n",
    "# Combine cleaned Janitors + other roles\n",
    "df_clean = pd.concat([janitors_clean, non_janitors], ignore_index=True)\n",
    "\n",
    "print(\"Before:\", len(df_merged))\n",
    "print(\"After cleaning Janitors:\", len(df_clean))\n",
    "print(\"Janitors removed:\", len(janitors) - len(janitors_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "From the cell output, the uppper limit is 137k which is a more realistic max value for the janitor role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x=\"job_role\", y=\"salary_k\", data=df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "After applying unrealistic outlier treatment, the salary distributions for janitor job role are is more stable and reasonable and is now a better representation of typical values. Extreme high points beyond the 1.5×IQR range were removed, reducing skew and making the medians clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "Why I will use the Median as the Main Average\n",
    "In this dataset, salaries vary widely within the same feature groups (job roles, industries, education levels). This means the distribution of salaries is often skewed, with extreme values (outliers) pulling the mean upwards or downwards.\n",
    "\n",
    "Mean (arithmetic average): Sensitive to outliers; can misrepresent the “typical” salary if a few very high/low salaries exist.\n",
    "\n",
    "Median (50th percentile): Robust to outliers; represents the middle point of the distribution.\n",
    "\n",
    "Therefore, for all central tendency analysis, I use the median to reflect the “typical” person within each filter. This ensures more reliable comparisons when identifying highest/lowest salaries or grouping by features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "## What is the highest paying job for the web industry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "First lets filter out dataset to only include the web industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_web = df_clean[df_clean[\"industry\"] == \"WEB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "Next I will group the respective jobs together and take the median value of them, we do this to get the median pay of each job. This is to ensure the average is not skewed by outliers as mean often times causes skewed data. We will also review the sample size of each job role to ensure each job role has sufficient representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by job_role in Web industry\n",
    "web_roles_stats = (df_web.groupby('job_role', observed=True)['salary_k'].agg(median_salary='median', count='size').sort_values(by='median_salary', ascending=False))\n",
    "top1_role_web = web_roles_stats.head(1)\n",
    "\n",
    "display(web_roles_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "Here we can see that the sample size for each job role is similar therefore we can compare the jobs without any issues. Lets plot the table to a bar chart to view the data with clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already grouped and calculated medians separately\n",
    "viz.plot_bar(df=web_roles_stats.reset_index(), x_col='job_role', y_col='median_salary',title=\"Median Salary by Job Role in Web Industry\")\n",
    "\n",
    "display(top1_role_web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {},
   "source": [
    "__Summary:__  \n",
    "\n",
    "From the bar chart, we can clearly see that the job role with the highest median pay of 147k in the web industry is CEO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "##  Rank the top 10 jobs roles with the highest salary for all the industry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "Since this time we do not need to analyse a specific industry, we can procceed to the group by step immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by industry + job_role and compute median salary + counts\n",
    "role_salary_stats = (\n",
    "    df_clean.groupby(['industry', 'job_role'], observed=True)['salary_k']\n",
    "    .agg(median_salary='median', count='size')\n",
    "    .reset_index()\n",
    "    .sort_values(by='median_salary', ascending=False)\n",
    ")\n",
    "\n",
    "# Select the top 10 roles by median salary across all industries\n",
    "top10_roles_by_industry = role_salary_stats.head(10).copy()\n",
    "\n",
    "# Create new combined column\n",
    "top10_roles_by_industry['role_label'] = (\n",
    "    top10_roles_by_industry['industry'].astype(str) \n",
    "    + ' - ' + \n",
    "    top10_roles_by_industry['job_role'].astype(str)\n",
    ")\n",
    "\n",
    "# Drop the original two columns\n",
    "top10_roles_by_industry = top10_roles_by_industry.drop(columns=['industry', 'job_role'])\n",
    "\n",
    "# Reorder columns so role_label is first\n",
    "cols = ['role_label'] + [c for c in top10_roles_by_industry.columns if c != 'role_label']\n",
    "\n",
    "top10_roles_by_industry = top10_roles_by_industry[cols]\n",
    "\n",
    "top10_roles_by_industry.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bar(df=top10_roles_by_industry.reset_index(), x_col='role_label', y_col='median_salary',title=\"Top 10 Job Roles across all industries by Median Salary\")\n",
    "\n",
    "display(top10_roles_by_industry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "__Summary:__  \n",
    "\n",
    "These are the top 10 jobs ranked by median salary across all industries with the highest paid job roles on the left and the lowest on the right side. (Graphical view)\n",
    "\n",
    "These are the top 10 jobs ranked by median salary across all industries with the highest paid job roles at the top and the lowest at the bottom. (Table view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122",
   "metadata": {},
   "source": [
    "## Which of the industries has the highest salary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "This time since we are only comparing across industries which contains a range of jobs, we need to have a robust method to compare the average salary of each industry. So what I will do is compare is highest lowest and median salary across all industries to get a more overarching view of the salaries of each type of individual i.e. median -> Typical employee, lowest -> lowest paying job-role & the highest -> best paying job-role in the industry. This should help the government understand which industries allow for growth of the employees and which industries have stagnant pay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each industry, get lowest, median, and highest job-role medians\n",
    "industry_salary_profile = (\n",
    "    df_clean.groupby('industry', observed=True)['salary_k']\n",
    "    .agg(count='size', lowest_salary='min', median_salary='median', highest_salary='max')\n",
    "    .reset_index()\n",
    "    .sort_values(by='median_salary', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate delta (growth potential)\n",
    "industry_salary_profile['salary_delta'] = industry_salary_profile['highest_salary'] - industry_salary_profile['lowest_salary']\n",
    "industry_salary_profile.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_clustered_bars(\n",
    "    df=industry_salary_profile,\n",
    "    x_col='industry',\n",
    "    y_cols=['median_salary', 'highest_salary', 'lowest_salary','salary_delta'],\n",
    "    title=\"Median Salary, Highest Salary & Salary Delta by Industry\"\n",
    ")\n",
    "\n",
    "# Display the industry salary profile\n",
    "display(industry_salary_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "Here we can see that the sample size for each of the industries are very similar therefore we can compare without any additional steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "__Summary:__  \n",
    "\n",
    "This clustered bar chart tells us quite a lot of information. We can observe that the typical employee in the Oil & Finance industry get paid the most at 128k. However, to differenciate between the two industries to find the \"highest\" paying we can then compare the highest pay, lowest pay & salary delta. Between the two industries, the oil industry has the highest salary at 301k as compared to the finance industries highest salary of 294k. Furthermore, when comparing the lowest salary, we can see that there is only a small difference of 1k but the oil industry is better paying at 37k as compared to the finance industry's 36k. When comparing the salary delta (Potential for growth) we can see that thee oil industry comes out ahead again at 264k compared to the finance industries 258k. Therefore, in conclusion typically for most employees, either the finance or oil industry will lead to the highest pay. However, if selecting a singular highest paying industry, the oil industry is the best paying industry when comparing all different levels of pay as well as the potential for income growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "## Which job has the lowest pay?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "As we are only comparing job roles this time, we will group by and only compare the job role salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each job_role, get count, min, median, max salary\n",
    "jobrole_salary_profile = (\n",
    "    df_clean.groupby('job_role', observed=True)['salary_k']\n",
    "    .agg(count='size',\n",
    "         lowest_salary='min',\n",
    "         median_salary='median',\n",
    "         highest_salary='max')\n",
    "    .reset_index()\n",
    "    .sort_values(by='median_salary', ascending=True)  # sort ascending for lowest pay\n",
    ")\n",
    "\n",
    "# The job with the lowest pay (first row)\n",
    "lowest_pay_job = jobrole_salary_profile.head(1)\n",
    "\n",
    "display(jobrole_salary_profile)  # full table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_clustered_bars(\n",
    "    df=jobrole_salary_profile,\n",
    "    x_col='job_role',\n",
    "    y_cols=['median_salary', 'highest_salary', 'lowest_salary'],\n",
    "    title=\"Median Salary, Highest Salary & Lowest Salary by Job Role\"\n",
    ")\n",
    "display(lowest_pay_job)          # just the lowest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "__Summary:__  \n",
    "\n",
    "From the graph and by sorting the df, we can see that the lowest paying job is the janitor role. It is the lowest paying across all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "## Which industries have the lowest pay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each industry, get count, min, median, max salary\n",
    "industry_salary_profile = (\n",
    "    df_clean.groupby('industry', observed=True)['salary_k']\n",
    "    .agg(count='size',\n",
    "         lowest_salary='min',\n",
    "         median_salary='median',\n",
    "         highest_salary='max')\n",
    "    .reset_index()\n",
    "    .sort_values(by='median_salary', ascending=True)  # sort ascending for lowest pay\n",
    ")\n",
    "\n",
    "# The industry with the lowest pay (first row)\n",
    "lowest_pay_industry = industry_salary_profile.head(1)\n",
    "\n",
    "display(industry_salary_profile)  # full table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_clustered_bars(\n",
    "    df=industry_salary_profile,\n",
    "    x_col='industry',\n",
    "    y_cols=['median_salary', 'highest_salary', 'lowest_salary'],\n",
    "    title=\"Median Salary, Highest Salary & Lowest Salary by Industry\"\n",
    ")\n",
    "display(lowest_pay_industry)          # just the lowest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "__Summary:__  \n",
    "\n",
    "The lowest paying industry is the education industry. It has the lowest min, median and maximum pay among all industries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "## Given that the median salary per year is $114,000\n",
    "- Which industry has the highest percentage of people who are below the median salary?\n",
    "- What are the job roles that are below the median salary? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "### Which industry has the highest percentage of people who are below the median salary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the cleaned DataFrame for filtering\n",
    "df_industry_filtered = df_clean.copy()\n",
    "\n",
    "# Fliter the rows by salary_k < 114\n",
    "df_industry_filtered['below_median'] = df_clean['salary_k'] < 114\n",
    "\n",
    "# Group by industry and agg the count & sum\n",
    "industry_below = (\n",
    "    df_industry_filtered.groupby(\"industry\")['below_median']\n",
    "    .agg(['count','sum'])  # count = total, sum = below median\n",
    ")\n",
    "\n",
    "# Calculate the percentage of people below median salary in each industry\n",
    "industry_below['%_below_median'] = ((industry_below['sum'] / industry_below['count']) * 100).round(2)\n",
    "\n",
    "# Sort descending to find industry with highest % below median\n",
    "industry_below_sorted = industry_below.sort_values(by=\"%_below_median\", ascending=False)\n",
    "\n",
    "display(industry_below_sorted.head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {},
   "source": [
    "__Summary:__  \n",
    "\n",
    "The industry with the largest % of people below the median pay is the education industry with 66.88%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "### What are the job roles that are below the median salary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "I am assuming that the question is asking for the count & % as all the job roles are under the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the cleaned DataFrame for filtering\n",
    "df_job_filtered = df_clean.copy()\n",
    "\n",
    "# Filter rows below the median\n",
    "df_job_filtered['below_median'] = df_clean['salary_k'] < 114\n",
    "\n",
    "# Group by job role: total count and number below median\n",
    "role_below = (\n",
    "    df_job_filtered.groupby(\"job_role\")['below_median']\n",
    "    .agg(['count','sum'])   # count = total rows, sum = rows below median\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate percentage\n",
    "role_below['%_below_median'] = ((role_below['sum'] / role_below['count']) * 100).round(2)\n",
    "\n",
    "# Sort by percentage (descending)\n",
    "role_below = role_below.sort_values(by='%_below_median', ascending=False).reset_index(drop=True)\n",
    "\n",
    "display(role_below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "__Summary:__  \n",
    "\n",
    "The above table includes the job roles below the mediian salary as well as the % of the people with the job role who are under the median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {},
   "source": [
    "## Determine if there is a relationship between years of experience and salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "Since these are two numeric columns, we can easily plot a scatter graph to determine the trend visually without the need of statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.scatterplot(x=\"years_experience\", y=\"salary_k\", data=df_clean, alpha=0.3)\n",
    "plt.title(\"Years of Experience vs. Salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "The graph is very noisy and as such it is difficult to determine a trend or relationship due to the overlapping points. However, it does seem that there is a slight increase in the max salary as age increases however there is no irrefutable proof. Therefore, to prevent this current situation where the min and max points for each year is very far apart, I will take the median age of each age and also add in a best fit line to determine the type of relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Calculate median salary for each years_experience (across all roles/industries)\n",
    "median_salary_overall = (\n",
    "    df_clean.groupby(\"years_experience\")['salary_k']\n",
    "    .median()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Scatter the median points\n",
    "sns.scatterplot(\n",
    "    x=\"years_experience\", y=\"salary_k\",\n",
    "    data=median_salary_overall,\n",
    "    marker=\"o\", s=60, color=\"blue\", label=\"Median Salary\"\n",
    ")\n",
    "\n",
    "# Add regression line based only on medians\n",
    "sns.regplot(\n",
    "    x=\"years_experience\", y=\"salary_k\",\n",
    "    data=median_salary_overall,\n",
    "    scatter=False,          \n",
    "    ci=None,                \n",
    "    color=\"red\", \n",
    "    line_kws={\"lw\":2, \"alpha\":0.8}, \n",
    "    label=\"Best Fit Line\"\n",
    ")\n",
    "\n",
    "plt.title(\"Median Salary vs. Years of Experience (Best Fit Line)\")\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Median Salary (K)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# X = years of experience, y = median salary\n",
    "X = median_salary_overall['years_experience']\n",
    "y = median_salary_overall['salary_k']\n",
    "\n",
    "# Fit a straight line (degree=1)\n",
    "slope, intercept = np.polyfit(X, y, 1)\n",
    "\n",
    "print(f\"Intercept: {intercept:.2f}\")\n",
    "print(f\"Slope (gradient): {slope:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "From the graph of median salary vs years of experience that has a best fitline, we can determine there is a direct relationship between the two variables. As the years of experience increases, the median salary increases by 1.94k. The starting median base pay at 0 years of experience being ~91K. Next I will use pearsonr to ensure the correlation is high as well as if the % chance of the the correlation occuring by chnace is low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Specify the columns for correlation\n",
    "x = df_clean['years_experience']\n",
    "y = df_clean['salary_k']\n",
    "\n",
    "# compute Pearson correlation\n",
    "corr_coef, p_value = pearsonr(x, y)\n",
    "\n",
    "print(f\"Pearson correlation coefficient: {corr_coef:.3f}\")\n",
    "print(f\"P-value: {p_value:.3e}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"The correlation is statistically significant (p < 0.05).\")\n",
    "else:\n",
    "    print(\"No statistically significant correlation detected (p ≥ 0.05).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "The Pearson correlation between years of experience and salary is 0.374, indicating a moderate positive relationship. The p-value is effectively zero, showing that this correlation is statistically significant and unlikely due to chance. This confirms that salaries generally increase as years of experience rise, consistent with the observed best-fit line trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {},
   "source": [
    "__Summary:__  \n",
    "\n",
    "Salaries show a clear upward trend with experience, starting at around 91k for entry-level roles and increasing by roughly 1.94k per year. The Pearson correlation coefficient of 0.374 confirms a moderate positive relationship, with a p-value effectively zero, meaning the correlation is statistically significant and unlikely due to chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "## Is there a relationship between education and salary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156",
   "metadata": {},
   "source": [
    "This is slightly different as education is an ordinal categorical column and salary is a numeric column so to determine the relationship we need to either encode the labels in order least to most educated or we can use a barchart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the different unique values in education\n",
    "education_counts = df_clean['education'].value_counts(dropna=False)\n",
    "display(education_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define natural order\n",
    "edu_order = {\n",
    "    \"NONE\": 0,\n",
    "    \"HIGH_SCHOOL\": 1,\n",
    "    \"BACHELORS\": 2,\n",
    "    \"MASTERS\": 3,\n",
    "    \"DOCTORAL\": 4\n",
    "}\n",
    "\n",
    "df_encoded = df_clean.copy()\n",
    "\n",
    "# Map to ordered integers\n",
    "df_encoded['education_encoded'] = df_clean['education'].map(edu_order)\n",
    "\n",
    "# Spearman correlation\n",
    "corr = df_encoded['education_encoded'].corr(df_encoded['salary_k'], method='spearman')\n",
    "print(\"Spearman correlation:\", round(corr, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159",
   "metadata": {},
   "source": [
    "The Spearman correlation between education and salary was 0.388, which indicates a weak-to-moderate positive relationship. This suggests that while higher education is associated with higher salaries, other factors such as job role, industry, and years of experience play a larger role in determining compensation. Let us verify the statistical analysis with a visual graph to ensure this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median salary per education group\n",
    "edu_salary = (\n",
    "    df_clean.groupby(\"education\")['salary_k']\n",
    "    .median()\n",
    "    .reindex([\"NONE\",\"HIGH_SCHOOL\",\"BACHELORS\",\"MASTERS\",\"DOCTORAL\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"education\", y=\"salary_k\", data=edu_salary, palette=\"viridis\")\n",
    "\n",
    "plt.title(\"Median Salary by Education Level\")\n",
    "plt.xlabel(\"Education Level\")\n",
    "plt.ylabel(\"Median Salary (in $000s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161",
   "metadata": {},
   "source": [
    "The bar chart of median salaries by education level shows a clear upward trend: employees with higher education levels generally earn more. The largest salary increase occurs between High School/None and Bachelor’s degree holders. However, while the correlation is positive, the progression is not uniform — salaries for Master’s and Doctoral levels are closer together. This visual evidence supports the Spearman correlation of 0.388, confirming a weak-to-moderate positive relationship between education and salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {},
   "source": [
    "__Summary:__ \n",
    "\n",
    "Education level shows a weak-to-moderate positive relationship with salary (Spearman ρ = 0.388). Salaries rise notably from High School/None to Bachelor’s, but the gap between Master’s and Doctoral degrees is smaller. This suggests that while education contributes to higher pay, other factors like role, industry, and experience have greater influence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163",
   "metadata": {},
   "source": [
    "## Does the major they studied affect the salary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164",
   "metadata": {},
   "source": [
    "This is once again a different situation, this is becuase there is no clear order to the major column as its nominal. Therefore, we have to visually confirm if any specific major leads to higher pay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute median salary per major and sort descending\n",
    "major_order = (\n",
    "    df_clean.groupby(\"major\")['salary_k']\n",
    "    .median()\n",
    "    .sort_values(ascending=False)\n",
    "    .index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the boxplot and barplot side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18,6), sharey=True)\n",
    "\n",
    "# --- Barplot: Median salary per major (ranked) ---\n",
    "sns.barplot(\n",
    "    x=\"major\", y=\"salary_k\",\n",
    "    data=df_clean,\n",
    "    estimator=\"median\", palette=\"viridis\",\n",
    "    order=major_order,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Median Salary by Major (Ranked)\")\n",
    "axes[0].set_xlabel(\"Major\")\n",
    "axes[0].set_ylabel(\"Salary (k)\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# --- Boxplot: Salary distribution per major (ranked) ---\n",
    "sns.boxplot(\n",
    "    x=\"major\", y=\"salary_k\",\n",
    "    data=df_clean,\n",
    "    palette=\"Set2\",\n",
    "    order=major_order,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Salary Distribution by Major (Ranked)\")\n",
    "axes[1].set_xlabel(\"Major\")\n",
    "axes[1].set_ylabel(\"\")  # hide duplicate y-label\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167",
   "metadata": {},
   "source": [
    "__Summary:__  \n",
    "\n",
    "After analyzing the relationship between major and salary using both ranked barplots of the median and boxplots of the distribution. The ranked barplot revealed that majors such as Engineering, Business, and Math had the highest median salaries, while majors such as Biology, Literature, and None were positioned towards the lower end. However, the boxplot showed that there was substantial overlap in salary distributions across majors, with many individuals in lower-ranked majors earning salaries comparable to those in higher-ranked majors. From this, I determined that while major does have an influence on salary, the effect is not strong enough to be considered a decisive factor on its own, and other variables such as job role, industry, and experience must also be taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168",
   "metadata": {},
   "source": [
    "# Feature Engineering & Data Preparation and Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Calculate median salary for distance (across all roles/industries)\n",
    "median_salary_overall = (\n",
    "    df_clean.groupby(\"distance_from_cbd\")['salary_k']\n",
    "    .median()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Scatter the median points\n",
    "sns.scatterplot(\n",
    "    x=\"distance_from_cbd\", y=\"salary_k\",\n",
    "    data=median_salary_overall,\n",
    "    marker=\"o\", s=60, color=\"blue\", label=\"Median Salary\"\n",
    ")\n",
    "\n",
    "# Add regression line based only on medians\n",
    "sns.regplot(\n",
    "    x=\"distance_from_cbd\", y=\"salary_k\",\n",
    "    data=median_salary_overall,\n",
    "    scatter=False,          \n",
    "    ci=None,                \n",
    "    color=\"red\", \n",
    "    line_kws={\"lw\":2, \"alpha\":0.8}, \n",
    "    label=\"Best Fit Line\"\n",
    ")\n",
    "\n",
    "plt.title(\"Median Salary vs. distance_from_cbd (Best Fit Line)\")\n",
    "plt.xlabel(\"distance_from_cbd\")\n",
    "plt.ylabel(\"Median Salary (K)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# X = years of experience, y = median salary\n",
    "X = median_salary_overall['distance_from_cbd']\n",
    "y = median_salary_overall['salary_k']\n",
    "\n",
    "# Fit a straight line (degree=1)\n",
    "slope, intercept = np.polyfit(X, y, 1)\n",
    "\n",
    "print(f\"Intercept: {intercept:.2f}\")\n",
    "print(f\"Slope (gradient): {slope:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170",
   "metadata": {},
   "source": [
    "During EDA, we did not check this column. Lets retain this as it provides a inverse relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute median salary per major and sort descending\n",
    "company_id_order = (\n",
    "    df_clean.groupby(\"company_id\")['salary_k']\n",
    "    .median()\n",
    "    .sort_values(ascending=False)\n",
    "    .index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the boxplot and barplot side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18,6), sharey=True)\n",
    "\n",
    "# --- Barplot: Median salary per major (ranked) ---\n",
    "sns.barplot(\n",
    "    x=\"company_id\", y=\"salary_k\",\n",
    "    data=df_clean,\n",
    "    estimator=\"median\", palette=\"viridis\",\n",
    "    order=company_id_order,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Median Salary by company_id (Ranked)\")\n",
    "axes[0].set_xlabel(\"company_id\")\n",
    "axes[0].set_ylabel(\"Salary (k)\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# --- Boxplot: Salary distribution per major (ranked) ---\n",
    "sns.boxplot(\n",
    "    x=\"company_id\", y=\"salary_k\",\n",
    "    data=df_clean,\n",
    "    palette=\"Set2\",\n",
    "    order=company_id_order,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Salary Distribution by company_id (Ranked)\")\n",
    "axes[1].set_xlabel(\"company_id\")\n",
    "axes[1].set_ylabel(\"\")  # hide duplicate y-label\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {},
   "source": [
    "Plot is a little cramped, but we can see that company id has no direct or inverse relationship. Therefore we will drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_split = catalog.load(\"employee_dataset_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Only numeric columns\n",
    "numeric_cols = pre_split.select_dtypes(include=['int64', 'int8', 'float64']).columns\n",
    "corr = pre_split[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(corr, cmap=\"coolwarm\", annot=False, center=0)\n",
    "plt.title(\"Correlation Matrix of Numeric Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176",
   "metadata": {},
   "source": [
    "I introduced a handcrafted score feature that prescores individuals before model training. The purpose was to give the model an additional signal that captures a holistic assessment of whether a person is in a “better” or “worse” state across multiple features. This helps the model move beyond narrow, isolated ranges of individual inputs and instead learn from a more aggregated, informative indicator. By doing so, the model is allowed to capture slightly more variance in the data.\n",
    "\n",
    "Initially, I considered adding multiple binary flags such as “is educated” vs. “not educated”. However, I realized this was redundant because many features already carry ordinal information (e.g., education level, job category, industry role, field of study). Instead, I converted these categorical variables into ordinal numeric representations, making the hierarchy explicit:\n",
    "\n",
    "0 = None\n",
    "\n",
    "1 = High school\n",
    "\n",
    "2 = Bachelor’s degree\n",
    "\n",
    "3 = Master’s degree\n",
    "\n",
    "4 = PhD\n",
    "… and so on.\n",
    "\n",
    "This approach is especially beneficial because the final model layer is linear. Linear layers are much better at handling numeric, ordered inputs than raw categorical encodings. By supplying ordinal encodings, the model does not have to infer the ordering relationship during training—it is already encoded directly in the input space. This ensures that the model can correctly interpret “higher is better” patterns across education, job roles, or other ranked features.\n",
    "\n",
    "Additionally, I optimized storage by downcasting these scores to int8, since all ordinal values fit within a range of 0–8. This reduces memory overhead and makes the dataset more compact without losing information.\n",
    "\n",
    "Overall, this preprocessing strategy improves the model’s efficiency and interpretability by:\n",
    "\n",
    "Reducing feature ambiguity – ordinal relationships are explicitly defined rather than learned indirectly.\n",
    "\n",
    "Supporting variance capture – the handcrafted state score provides a more global signal of socioeconomic position.\n",
    "\n",
    "Enhancing efficiency – compact numeric encoding (int8) minimizes storage and speeds up computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177",
   "metadata": {},
   "source": [
    "# ML Model Development "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to plot metrics & graphs\n",
    "def evaluate_model(metrics, predictions, history, title_prefix=\"Model\"):\n",
    "    \"\"\"\n",
    "    Show metrics, loss curves, prediction scatter, and residuals.\n",
    "    Args:\n",
    "        metrics (dict): Linear Regression {\"r2\": ..., \"mae\": ..., \"rmse\": ...}\n",
    "        predictions (pd.DataFrame): must contain [\"y_true\", \"y_pred\"]\n",
    "        history (dict): {\"train_loss\": [...], \"val_loss\": [...]}\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Print metrics\n",
    "    print(f\"\\n{title_prefix} Performance Metrics\")\n",
    "    print(\"-\" * 40)\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k.upper():<6}: {v:.4f}\")\n",
    "\n",
    "    # 2. Create subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # Loss curves\n",
    "    axes[0].plot(history.get(\"train_loss\", []), label=\"Train Loss\")\n",
    "    axes[0].plot(history.get(\"val_loss\", []), label=\"Validation Loss\")\n",
    "    axes[0].set_title(\"Loss Curves\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"MSE Loss\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Predictions vs True\n",
    "    sns.scatterplot(\n",
    "        x=predictions[\"y_true\"], \n",
    "        y=predictions[\"y_pred\"], \n",
    "        alpha=0.5, ax=axes[1]\n",
    "    )\n",
    "    min_val = min(predictions[\"y_true\"].min(), predictions[\"y_pred\"].min())\n",
    "    max_val = max(predictions[\"y_true\"].max(), predictions[\"y_pred\"].max())\n",
    "    axes[1].plot([min_val, max_val], [min_val, max_val], \"r--\")\n",
    "    axes[1].set_title(\"Predicted vs True\")\n",
    "    axes[1].set_xlabel(\"True Salary (k)\")\n",
    "    axes[1].set_ylabel(\"Predicted Salary (k)\")\n",
    "\n",
    "    # Residuals\n",
    "    residuals = predictions[\"y_true\"] - predictions[\"y_pred\"]\n",
    "    sns.scatterplot(\n",
    "        x=predictions[\"y_true\"], \n",
    "        y=residuals, \n",
    "        alpha=0.5, ax=axes[2]\n",
    "    )\n",
    "    axes[2].axhline(0, color=\"red\", linestyle=\"--\")\n",
    "    axes[2].set_title(\"Residuals\")\n",
    "    axes[2].set_xlabel(\"True Salary (k)\")\n",
    "    axes[2].set_ylabel(\"Residual (True - Pred)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179",
   "metadata": {},
   "source": [
    "## Model Selection and Justification\n",
    "Model type will be selected in non spark workflow and converted to pyspark variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_metrics = catalog.load(\"rf_metrics\")\n",
    "rf_predictions = catalog.load(\"rf_predictions\")\n",
    "rf_history = catalog.load(\"rf_history\")\n",
    "\n",
    "evaluate_model(rf_metrics, rf_predictions, rf_history, title_prefix=\"Simple Random Forrest Regressor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_metrics = catalog.load(\"lr_metrics\")\n",
    "lr_predictions = catalog.load(\"lr_predictions\")\n",
    "lr_history = catalog.load(\"lr_history\")\n",
    "\n",
    "evaluate_model(lr_metrics, lr_predictions, lr_history, title_prefix=\"Simple Linear Regression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_metrics = catalog.load(\"nn_metrics\")\n",
    "nn_predictions = catalog.load(\"nn_predictions\")\n",
    "nn_history = catalog.load(\"nn_history\")\n",
    "\n",
    "evaluate_model(nn_metrics, lr_predictions, nn_history, title_prefix=\"Simple Nueral Network\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183",
   "metadata": {},
   "source": [
    "Doing the same for the spark version of the same models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_metrics_spark = catalog.load(\"nn_metrics_spark\")\n",
    "nn_predictions_spark = catalog.load(\"nn_predictions_spark\")\n",
    "nn_history_spark = catalog.load(\"nn_history_spark\")\n",
    "\n",
    "evaluate_model(nn_metrics_spark, nn_predictions_spark, nn_history_spark, title_prefix=\"Spark Fed Neural Network\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results directly from catalog\n",
    "lr_results = catalog.load(\"linear_regression_results\")\n",
    "feature_cols = [\n",
    "    \"years_experience\", \"distance_from_cbd\",\n",
    "    \"education_level\", \"job_role_rank\",\n",
    "    \"industry_score\", \"major_score\", \"handcrafted_score\"\n",
    "]\n",
    "\n",
    "viz.show_model_results(lr_results, feature_names=feature_cols, title=\"Spark Linear Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results directly from catalog\n",
    "lr_results = catalog.load(\"random_forest_results\")\n",
    "feature_cols = [\n",
    "    \"years_experience\", \"distance_from_cbd\",\n",
    "    \"education_level\", \"job_role_rank\",\n",
    "    \"industry_score\", \"major_score\", \"handcrafted_score\"\n",
    "]\n",
    "\n",
    "viz.show_model_results(lr_results, feature_names=feature_cols, title=\"Spark Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187",
   "metadata": {},
   "source": [
    "From our initial model selection of Linear Regression, Random Forest Regressor, and a simple Neural Network with embeddings, it can be determined that as a baseline the three models perform quite similarly. The Neural Network performed the best, likely because it is able to capture more nuanced non-linear patterns through extended training, while the Random Forest was the weakest. Therefore, the meaningful comparison is between the Linear Regression and the Neural Network.\n",
    "\n",
    "From the metrics, the R² score of the NN is slightly better at ~0.76 compared to ~0.74 for the Linear model. The difference is modest and might not always justify the added complexity, but the NN has several other benefits. Importantly, because the dataset mixes categorical and numeric features, embedding layers in the NN handle high-cardinality categorical variables more effectively than one-hot encoding, which benefits both regression and tree-based models.\n",
    "\n",
    "When considering explainability, a Linear Regression model is highly transparent and easy to communicate to business users, project sponsors, and the CTO. However, this advantage can be partially offset by the fact that the NN here is relatively simple (two hidden layers), making it easier to explain than a deep or highly complex architecture. In addition, tuning a Linear Regression model is limited (e.g., ridge, lasso), while a Neural Network offers a much broader space for optimization and tuning, often leading to stronger final performance.\n",
    "\n",
    "From a systems and deployment perspective, the NN has further advantages. The model is implemented in Torch, which integrates seamlessly with PySpark, making migration to distributed data processing much easier. This also reduces the drawbacks of longer training times by leveraging cluster-scale training. Torch also has built-in support for GPU acceleration (including AMD GPUs, which are often unsupported in other libraries), meaning it can handle larger datasets more efficiently. When scaling to much larger data sizes, the NN would be more robust than the Linear Regression baseline due to Torch’s deployability and parallelism. Finally, after data drift, retraining a Torch model on new data is straightforward and efficient, making it a practical long-term choice.\n",
    "\n",
    "Overall, while the Linear Regression model remains more interpretable, the Neural Network is the better long-term choice after weighing trade-offs in explainability, training time, scalability, and future deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188",
   "metadata": {},
   "source": [
    "As the best performing models are the pytorch NN as they both explain the varience in the dataset across all models. I will use use pytorch for both workflows to ensure comparison between speed and resources is accurate. Therefore, the final workflows will be for nonspark pandas -> pytorch and for spark workflow it will be spark -> pytorch(torchdistributor via spark if possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189",
   "metadata": {},
   "source": [
    "## Model Evaluation and Interpretation Post Tuning\n",
    "Since we decided on the NN for both workflows, we shall compare the 2 final models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_metrics_spark_finetuned = catalog.load(\"nn_metrics_spark_finetuned\")\n",
    "nn_predictions_spark_finetuned = catalog.load(\"nn_predictions_spark_finetuned\")\n",
    "nn_history_spark_finetuned = catalog.load(\"nn_history_spark_finetuned\")\n",
    "\n",
    "evaluate_model(nn_metrics_spark, nn_predictions_spark_finetuned, nn_history_spark_finetuned, title_prefix=\"Spark Fed Neural Network Tuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_metrics_nonspark_finetuned = catalog.load(\"nn_metrics_nonspark_finetuned\")\n",
    "nn_predictions_nonspark_finetuned = catalog.load(\"nn_predictions_nonspark_finetuned\")\n",
    "nn_history_nonspark_finetuned = catalog.load(\"nn_history_nonspark_finetuned\")\n",
    "\n",
    "evaluate_model(nn_metrics_nonspark_finetuned, nn_predictions_nonspark_finetuned, nn_history_nonspark_finetuned, title_prefix=\"Tuned nonspark NN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192",
   "metadata": {},
   "source": [
    "**Interpretation & Evaluation of Tuned Models**\n",
    "\n",
    "After fine-tuning the models for an additional 3 epochs with small hyperparameter grid searches, the results show **no meaningful improvement**. Both the metrics and the graphs indicate that the models have **converged**, with loss curves plateauing and further training failing to reduce error significantly. Lets evaluate the best overall model as the second model is similar in performance.\n",
    "\n",
    "**Performance Metrics**  \n",
    "- **R² = 0.7617** → explains ~76% of the variance, but still leaves a large portion unexplained.  \n",
    "- **MAE ≈ 15.3k** and **RMSE ≈ 18.9k** → prediction errors are large relative to typical salaries (~20–25% of the median).  \n",
    "- Suitable for **broad salary band estimation**, but **not for individual-level predictions**.  \n",
    "\n",
    "**Graphical Evaluation**\n",
    "\n",
    "*Loss Curves*  \n",
    "- Training loss decreases slightly, but validation loss flattens early.  \n",
    "- Indicates the model has reached maximum capacity given the data; further epochs or minor hyperparameter changes yield **no significant gain**.  \n",
    "\n",
    "*Predicted vs. True Salaries*  \n",
    "- Predictions align well with the diagonal for **average salaries**, showing generalization for typical employees.  \n",
    "- **High salaries** are systematically underestimated → regression to the mean.  \n",
    "- **Mid-range salaries** show instability → scattered above/below the diagonal due to missing explanatory features.  \n",
    "\n",
    "*Residuals*  \n",
    "- Residuals show systematic bias, not randomness:  \n",
    "  - **High earners**: underpredicted (positive residuals).  \n",
    "  - **Mid earners**: wide, noisy spread.  \n",
    "  - **Low earners**: relatively accurate.  \n",
    "**Interpretation**  \n",
    "- Models exhibit **regression to the mean** → biased toward the “average” case.  \n",
    "- This reflects a **bias–variance trade-off**:  \n",
    "  - Low variance → stable mean-like predictions.  \n",
    "  - High bias → failure to capture outliers or nuanced differences.  \n",
    "- Even the neural network with embeddings plateaued, showing **added complexity does not overcome limited data signal**.  \n",
    "\n",
    "**Limitations**  \n",
    "\n",
    "*Data-related*  \n",
    "- Missing key variables: company size, certifications, career stage, macroeconomic factors.  \n",
    "- Potential **response bias** in survey-based data.  \n",
    "\n",
    "*Model-related*  \n",
    "- Features explain only part of salary variance.  \n",
    "- Models generalize to the average, but fail for mid/high earners where variance is high.  \n",
    "**Recommendations**  \n",
    "- **Use case**: Apply for **broad salary grouping or benchmarking**, not precise forecasts.  \n",
    "- **Data enrichment**: Add richer features (seniority, company prestige, negotiation skills, certifications).  \n",
    "- **Awareness**: Salaries are influenced by **non-quantifiable factors** (networking, mobility, negotiation), limiting prediction accuracy.  \n",
    "\n",
    "**Summary**  \n",
    "The models perform adequately for general salary trends but plateau quickly, regress toward the mean, and systematically underestimate high salaries.  \n",
    "Their usefulness is limited to estimating **broad ranges**, not precise predictions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193",
   "metadata": {},
   "source": [
    "# Comparison between PySpark and non-PySpark workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194",
   "metadata": {},
   "source": [
    "__Speed & Ease of Use__  \n",
    "For this project, Pandas was faster overall. The NONSPARK pipeline (4 nodes) completed in 163.72s with an average CPU usage of 23.38%, while the SPARK pipeline (4 nodes) took 194.07s with 34.02% CPU. Since the dataset was relatively small, Pandas handled the workload efficiently, with model training finishing within 1–2 minutes (linear regression even in seconds). By contrast, Spark introduced additional overhead for cluster initialization and distributed execution, which slowed things down for small-scale data.\n",
    "\n",
    "__Coding Complexity & Integration__  \n",
    "Pandas was easier to implement — operations are concise and intuitive on smaller datasets. However, PySpark’s strength lies in its integration with Kedro. Spark automatically initializes at the start of each run and shuts down afterward, managed through simple spark.yml configuration. This allows seamless scaling and ensures Spark only runs as long as needed, without requiring heavy manual setup.\n",
    "\n",
    "__Scalability & Resource Utilization__  \n",
    "While Spark was slower here, its distributed architecture makes it far more scalable. On datasets with billions of rows or when training complex models, Spark would significantly outperform Pandas by distributing computation across CPUs and GPUs. Tools like TorchDistributor can even shard deep learning model training across nodes, drastically reducing training time. Pandas, by contrast, is constrained by single-machine memory and compute limits. The run summary confirms this trade-off: Pandas was lighter on CPU (23.38%), while Spark pushed CPU utilization higher (34.02%) but still underutilized given the dataset size.\n",
    "\n",
    "__Recommendation__  \n",
    "For small-to-medium datasets, Pandas is the better choice due to its speed and simplicity. For large-scale or distributed workloads, PySpark is the clear winner. Kedro makes this transition straightforward, as Spark can be enabled with minor configuration and minimal changes to the training code, enabling future expansion to clusters or GPUs with minimal overhead.\n",
    "\n",
    "__Run Summary (Kedro Execution)__\n",
    "\n",
    "SPARK → 4 nodes | 194.07s | 34.02% CPU\n",
    "\n",
    "NONSPARK → 4 nodes | 163.72s | 23.38% CPU\n",
    "\n",
    "OTHER → 2 nodes | 52.36s | 81.35% CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195",
   "metadata": {},
   "source": [
    "# Recommended Strategies for Career Development "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196",
   "metadata": {},
   "source": [
    "__Career Development Recommendations for a 30-Year-Old Mid-Careerist (Web/AI/Data Engineering Background)__\n",
    "\n",
    "__1. Industry Transition: Automotive (AUTO)__\n",
    "- **Why (beyond money):**  \n",
    "  - The automotive industry is undergoing a major transformation with **electric vehicles (EVs), autonomous driving, and smart manufacturing**.  \n",
    "  - It provides a hands-on environment that aligns with his *fixing things* interest while leveraging his AI/data engineering background in predictive maintenance, computer vision for self-driving, or IoT-connected cars.  \n",
    "  - Compared to his previous Web/AI role, AUTO offers a **tangible, physical product** that he can see and touch, giving more satisfaction for someone who enjoys practical problem-solving.  \n",
    "\n",
    "__2. Skillsets Needed__\n",
    "- **AI & Data for AUTO:**  \n",
    "  - Machine learning for autonomous driving (computer vision, reinforcement learning).  \n",
    "  - IoT and sensor data pipelines (real-time telemetry from vehicles).  \n",
    "  - Predictive maintenance and manufacturing optimization.  \n",
    "- **Domain-specific knowledge:**  \n",
    "  - Basics of mechanical/electrical systems (EV batteries, automotive electronics).  \n",
    "  - Industry tools (CAN bus, ROS for robotics).  \n",
    "\n",
    "__3. How to Obtain These Skills__\n",
    "- **Courses & Certifications:**  \n",
    "  - *Udacity Self-Driving Car Nanodegree* (autonomous driving).  \n",
    "  - *Coursera – EV & Automotive Engineering Specializations*.  \n",
    "- **Practical Projects:**  \n",
    "  - Work on open-source autonomous driving frameworks (e.g., Apollo, Autoware).  \n",
    "  - Tinker with Arduino/Raspberry Pi + sensors to simulate vehicle telemetry (aligns with his hobby of fixing things).  \n",
    "\n",
    "__4. Career & Lifestyle Fit__\n",
    "- AUTO provides a **stable, innovative, and future-facing industry** with opportunities to grow as EV adoption accelerates.  \n",
    "- It ties directly to his **personal interest in mechanics** (fixing things) while still leveraging his **AI/Data Engineering expertise**.  \n",
    "- This alignment can improve **motivation, career satisfaction, and resilience** against burnout, since work feels meaningful and enjoyable.  \n",
    "\n",
    "__5. Actionable Roadmap__\n",
    "- **Short-term (0–6 months):**  \n",
    "  - Take an online EV/autonomous systems course.  \n",
    "  - Build a small side project (e.g., predictive model for car part failures).  \n",
    "- **Medium-term (6–18 months):**  \n",
    "  - Apply for *Data Engineer/AI Engineer roles in Automotive Tech* (e.g., Tesla, Rivian, or traditional OEMs adopting EV/AI).  \n",
    "- **Long-term (2–3 years):**  \n",
    "  - Specialize in a high-demand niche (EV battery AI, autonomous driving, or smart manufacturing).  \n",
    "\n",
    "__Summary__\n",
    "The **Automotive industry** is the best fit: it merges his **AI/data skills** with his **personal love of fixing things**. Unlike Health or Education, AUTO gives him a **hands-on, innovation-driven environment** that feels both practical and futuristic. This strategy is not only practical but also personally fulfilling, making it the strongest recommendation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
