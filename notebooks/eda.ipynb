{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Project Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kedro \n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set Kedro project path\n",
    "project_path = Path.cwd().parent\n",
    "\n",
    "# Bootstrap Kedro\n",
    "from kedro.framework.startup import bootstrap_project\n",
    "from kedro.framework.session import KedroSession\n",
    "\n",
    "bootstrap_project(project_path)\n",
    "session = KedroSession.create(project_path=project_path)\n",
    "context = session.load_context()\n",
    "catalog = context.catalog\n",
    "\n",
    "# Add src/ to Python path\n",
    "sys.path.append(str(project_path / \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) # Ignore FutureWarnings globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import full modules (for reload)\n",
    "import egt305_job_market_analysis.utils.viz as viz\n",
    "\n",
    "import importlib\n",
    "importlib.reload(viz)\n",
    "\n",
    "# Set custom plot style for consistency\n",
    "viz.set_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Data Injestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Datasets\n",
    "df_employee = catalog.load(\"employee_dataset\")\n",
    "df_salary = catalog.load(\"employee_salaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Employee Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Inspecting the employee dataset for basic information & statistics\n",
    "\n",
    "# Shape of the dataset\n",
    "print(f\"Dataset shape: {df_employee.shape}\")\n",
    "\n",
    "# 2. Preview first 5 rows\n",
    "display(df_employee.head())\n",
    "\n",
    "# 3. Column names and data types\n",
    "df_employee.info()\n",
    "\n",
    "# 4. Descriptive statistics for numerical and categorical features\n",
    "display(df_employee.describe(include='all'))\n",
    "\n",
    "# 5. Check for missing values\n",
    "missing_counts = df_employee.isnull().sum()\n",
    "missing_perc = (missing_counts / len(df_employee) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing Count': missing_counts, 'Missing %': missing_perc})\n",
    "display(missing_df[missing_df['Missing Count'] > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Initial issues or anomalies detected.\n",
    "\n",
    "- Column names are not in a standard format as they have upper & lower case\n",
    "- Data entries are unecessarily complex i.e. COMP37 as they could just be 37\n",
    "- Columns are not in the correct dtype\n",
    "- distanceFromCBD has a very large difference from 75% to MAX indicating high value outliers\n",
    "- missing data in multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Cleaning Employee Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial column names from data description\n",
    "# standardizing column names for consistency\n",
    "df_employee.rename(columns={\n",
    "    'jobId': 'job_id',\n",
    "    'companyId': 'company_id',\n",
    "    'jobRole': 'job_role',\n",
    "    'education': 'education',\n",
    "    'major': 'major',\n",
    "    'Industry': 'industry',\n",
    "    'yearsExperience': 'years_experience',\n",
    "    'distanceFromCBD': 'distance_from_cbd'\n",
    "}, inplace=True)\n",
    "\n",
    "df_employee.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "Fixed column names to be more standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check value counts for company_id including NaN\n",
    "company_counts = df_employee['company_id'].value_counts(dropna=False)\n",
    "\n",
    "display(company_counts)\n",
    "print(f\"Unique company_id count (including NaN): {df_employee['company_id'].nunique(dropna=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "There is a good data spread in the company_id column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Checking the various unique entries as well as ensuring the prefix is COMP for all, as well as keeping a before prefix drop state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remove 'COMP' prefix and convert to integer\n",
    "df_employee['company_id'] = (\n",
    "    df_employee['company_id']\n",
    "    .astype(str)\n",
    "    .str.replace('COMP', '', regex=False)\n",
    "    .replace('<NA>', pd.NA)  # make sure string '<NA>' is real missing value\n",
    "    .astype('Int64')  # nullable integer dtype just for eda purposes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check value counts for company_id including NaN\n",
    "company_counts = df_employee['company_id'].value_counts(dropna=False)\n",
    "\n",
    "display(company_counts)\n",
    "print(f\"Unique company_id count (including NaN): {df_employee['company_id'].nunique(dropna=False)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Fixed company id column to be more model friendly whilst maintaining all entries including NA (to be dealt with when handling missing or dupe data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Next lets ensure the job_id column follows the same rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# pattern check for job_id\n",
    "pattern_check_job = df_employee['job_id'].apply(\n",
    "    lambda x: pd.isna(x) or bool(re.match(r'^JOB\\d+$', str(x)))\n",
    ")\n",
    "print(f\"All non-null job_id match 'JOBxxxx...' format?: {pattern_check_job.all()}\")\n",
    "\n",
    "# Remove 'JOB' prefix and convert to nullable integer\n",
    "df_employee['job_id'] = (\n",
    "    df_employee['job_id']\n",
    "    .astype(str)\n",
    "    .str.replace('JOB', '', regex=False)\n",
    "    .replace(['<NA>', 'nan', 'NaN'], pd.NA)  # ensure true missing values\n",
    "    .astype('Int64')  # nullable integer dtype\n",
    ")\n",
    "\n",
    "df_employee.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all string columns\n",
    "string_cols = df_employee.select_dtypes(include='string').columns\n",
    "\n",
    "# Display unique values for each string column\n",
    "for col in string_cols:\n",
    "    temp_series = df_employee[col].astype(str)  # ensure string format for display\n",
    "    \n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(f\"Unique values: {temp_series.nunique(dropna=False)}\")\n",
    "    print(temp_series.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "- job_role column seems to have a good spread of job roles except for a sole exception which is the president job role but all roles are valid and do not have semantic overlap. Some missing data but will be handled later.\n",
    "\n",
    "- education column has a large amount of missing data which is labeled as NONE and NA. Good spread of data without any semantic overlap.\n",
    "\n",
    "- major column has a similar issue with education column with missing data with 2 different labels but aside from missing data, there is a good spread of data among the categories with no semantic overlap.\n",
    "\n",
    "- industry column has a similar issue with job_role column. One singular entry in a category i.e. governement. However, there is a good spread of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "We will treat NA and NONE as the same for those columns that have both appear at the same time. For the industry & job_role column which only have NA, as there is no clear category or large amount of missing rows, we will drop the entire row which has the missing data. For numeric columns as they only have <500 missing values, we will also just drop the entire row as they are not a significant portion of data as seen in initial inspection, this also includes the missing entries in job_id. This no tolerance for missing data will ensure the data is as complete as can be baring those columns which have a large portion of missing data which will be explicitly labeled as NONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns where NA/NaN should be treated as 'NONE'\n",
    "cols_na_to_none = ['education', 'major']\n",
    "\n",
    "for col in cols_na_to_none:\n",
    "    df_employee[col] = (\n",
    "        df_employee[col]\n",
    "        .replace(['NA', 'na', 'NaN', 'nan', '<NA>'], pd.NA)  # unify NA forms\n",
    "        .fillna('NONE')  # replace actual missing with 'NONE'\n",
    "    )\n",
    "\n",
    "# Preview changes\n",
    "for col in cols_na_to_none:\n",
    "    print(f\"\\n--- {col} ---\")\n",
    "    print(df_employee[col].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in job_role, or industry\n",
    "cols_drop_na = ['job_role', 'industry']\n",
    "\n",
    "before_drop = len(df_employee)\n",
    "df_employee = df_employee.dropna(subset=cols_drop_na)\n",
    "after_drop = len(df_employee)\n",
    "\n",
    "print(f\"Dropped {before_drop - after_drop} rows due to NA in {cols_drop_na}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing data in integer columns\n",
    "int_cols = df_employee.select_dtypes(include=['int64', 'Int64']).columns\n",
    "\n",
    "before_drop = len(df_employee)\n",
    "df_employee = df_employee.dropna(subset=int_cols)\n",
    "after_drop = len(df_employee)\n",
    "\n",
    "print(f\"Dropped {before_drop - after_drop} rows due to NA in integer columns: {list(int_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for remaining missing values in all columns\n",
    "missing_counts = df_employee.isna().sum()\n",
    "missing_perc = (missing_counts / len(df_employee) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing %': missing_perc\n",
    "}).sort_values(by='Missing Count', ascending=False)\n",
    "\n",
    "display(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "This means there is no longer any column with NA entry they are either specifically mapped to NONE or dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Next lets fix the datatypes of the string columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Check datatypes & memory usage of all columns\n",
    "df_employee.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "- Since we no longer have any nulls, nullable ints are no longer needed. In data preparation pipeline, we will ensure that missing data is dropped immediately and not converted to Int64 and then dropped later which would be less memory efficient (Done here to help with flow of EDA)\n",
    "\n",
    "- To ensure memory usage is minimal, we will convert string to categorical dtype\n",
    "\n",
    "- One specific case will be company_id as it is nominal and not ordinal, we will convert it to a categorical as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert company_id to category\n",
    "if 'company_id' in df_employee.columns:\n",
    "    df_employee['company_id'] = df_employee['company_id'].astype('category')\n",
    "\n",
    "# Convert string columns to category for efficient EDA\n",
    "for col in df_employee.select_dtypes(include='string').columns:\n",
    "    df_employee[col] = df_employee[col].astype('category')\n",
    "\n",
    "# Convert nullable Int64 columns to regular int64\n",
    "int_cols_nullable = df_employee.select_dtypes(include='Int64').columns\n",
    "df_employee[int_cols_nullable] = df_employee[int_cols_nullable].astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step: Check datatypes & memory usage of all columns\n",
    "df_employee.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "we can see a reduction from 72.5 MB to 35.3 MB. This is a good optimization method. Especially considering that in big data reducing the memory usage of the data will help with speed of all downstream tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Done for now with cleaning of Employee dataset. Analysis of data with graphs will be done after cleaning of salary dataset & merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Salary Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Inspecting the salary dataset for basic information & statistics\n",
    "\n",
    "# Shape of the dataset\n",
    "print(f\"Dataset shape: {df_salary.shape}\")\n",
    "\n",
    "# 2. Preview first 5 rows\n",
    "display(df_salary.head())\n",
    "\n",
    "# 3. Column names and data types\n",
    "df_salary.info()\n",
    "\n",
    "# 4. Descriptive statistics for numerical and categorical features\n",
    "display(df_salary.describe(include='all'))\n",
    "\n",
    "# 5. Check for missing values\n",
    "missing_counts = df_salary.isnull().sum()\n",
    "missing_perc = (missing_counts / len(df_salary) * 100).round(2)\n",
    "missing_df = pd.DataFrame({'Missing Count': missing_counts, 'Missing %': missing_perc})\n",
    "display(missing_df[missing_df['Missing Count'] > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Issues or anomalies with dataset\n",
    "\n",
    "- non standardized column names\n",
    "- complex job_id data entry\n",
    "- missing data\n",
    "- improper dtypes\n",
    "- Clear high value outlier in salaryInThousands column\n",
    "- min value is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Cleaning Salary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_salary.rename(columns={\n",
    "    'jobId': 'job_id',\n",
    "    'salaryInThousands': 'salary_k'\n",
    "}, inplace=True)\n",
    "\n",
    "# Preview to confirm\n",
    "df_salary.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Clean job_id (handle missing + strip JOB)\n",
    "df_salary['job_id'] = (\n",
    "    df_salary['job_id']\n",
    "    .astype('string')                         # work in string mode for replace\n",
    "    .str.replace('JOB', '', regex=False)      # remove 'JOB' prefix\n",
    "    .astype('Int64')                          # nullable integer dtype\n",
    ")\n",
    "\n",
    "# Preview to confirm\n",
    "df_salary[['job_id']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing job_id or salary_k\n",
    "before_drop = len(df_salary)\n",
    "df_salary = df_salary.dropna(subset=['job_id', 'salary_k'])\n",
    "after_drop = len(df_salary)\n",
    "\n",
    "print(f\"Dropped {before_drop - after_drop} rows with missing job_id or salary_k\")\n",
    "print(f\"Remaining rows: {after_drop}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking column dtype & memory usage\n",
    "df_salary.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to non-nullable int64\n",
    "df_salary['job_id'] = df_salary['job_id'].astype('int64')\n",
    "df_salary['salary_k'] = df_salary['salary_k'].astype('int64')\n",
    "\n",
    "print(df_salary.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking column dtype & memory usage\n",
    "df_salary.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Merged Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Merging both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare job_id counts and matches\n",
    "emp_ids = set(df_employee['job_id'])\n",
    "sal_ids = set(df_salary['job_id'])\n",
    "\n",
    "print(f\"Unique job_ids in employee dataset: {len(emp_ids)}\")\n",
    "print(f\"Unique job_ids in salary dataset:   {len(sal_ids)}\")\n",
    "\n",
    "# Check exact match\n",
    "print(f\"All employee job_ids in salary dataset? {emp_ids.issubset(sal_ids)}\")\n",
    "print(f\"All salary job_ids in employee dataset? {sal_ids.issubset(emp_ids)}\")\n",
    "\n",
    "# Find differences\n",
    "missing_in_salary = emp_ids - sal_ids\n",
    "missing_in_employee = sal_ids - emp_ids\n",
    "\n",
    "print(f\"Job_ids in employee but not in salary: {len(missing_in_salary)}\")\n",
    "print(f\"Job_ids in salary but not in employee: {len(missing_in_employee)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "Since there are missing ID's in both datasets we will use an inner join to ensure only ids that are present make it into the final merged dataset to ensure no new null values are introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track before merge\n",
    "rows_emp_before = len(df_employee)\n",
    "rows_sal_before = len(df_salary)\n",
    "\n",
    "# Inner join\n",
    "df_merged = df_employee.merge(df_salary, on='job_id', how='inner')\n",
    "\n",
    "# Track after merge\n",
    "rows_after = len(df_merged)\n",
    "\n",
    "print(f\"Rows in employee dataset before merge: {rows_emp_before}\")\n",
    "print(f\"Rows in salary dataset before merge:   {rows_sal_before}\")\n",
    "print(f\"Rows after inner join:                  {rows_after}\")\n",
    "\n",
    "print(f\"Rows lost from employee dataset: {rows_emp_before - rows_after}\")\n",
    "print(f\"Rows lost from salary dataset:   {rows_sal_before - rows_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the final merged dataset to ensure inner join worked as expected\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "Inner join went well and minimal data was lost from the initial 1,000,000 to 999479. Now we will move onto data cleaning on merged dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Cleaning Merged dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "#### Inspecting Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Inspecting the merged dataset for basic information & statistics\n",
    "\n",
    "# Shape of the dataset\n",
    "print(f\"Dataset shape: {df_merged.shape}\")\n",
    "\n",
    "# 2. Preview first 5 rows\n",
    "display(df_merged.head())\n",
    "\n",
    "# Descriptive statistics (excluding job_id but keeping salary_k) with thousands separator\n",
    "display(df_merged.drop(columns=['job_id']).describe(include='all').style.format(thousands=',', precision=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "The main thing needed to be done now is to handle outliers in the numerical & categorical columns. There is also a need to check for complete duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "#### Handling duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for full duplicate rows\n",
    "full_dupes = df_merged.duplicated().sum()\n",
    "print(f\"Full duplicate rows: {full_dupes}\")\n",
    "\n",
    "# Check for duplicate job_id values\n",
    "jobid_dupes = df_merged['job_id'].duplicated().sum()\n",
    "print(f\"Duplicate job_id count: {jobid_dupes}\")\n",
    "\n",
    "# Display the first few duplicate job_id entries if present\n",
    "if jobid_dupes > 0:\n",
    "    display(df_merged[df_merged['job_id'].duplicated(keep=False)].sort_values('job_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "#### Handling invalid outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "Lets go column by column when handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_categoricals(df=df_merged,cols=['job_role'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "From the graph, we can once again confirm, that there is a good spread of data however there is a clear singular outlier. Unfortunately, as there is no significant count of president category job_role, this means that this singular president may skew the averages of one specific industry or company or job role as he/she is not representative of the majority thus affecting eda later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the president outlier from job_role\n",
    "df_merged = df_merged[df_merged['job_role'] != 'PRESIDENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'PRESIDENT' from job_role if it exists\n",
    "df_merged['job_role'] = df_merged['job_role'].cat.remove_unused_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_categoricals(df=df_merged,cols=['education'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "This column has no major outliers, however there is a large number of NONE present however as they are explicitly labled it will not be an issue later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_categoricals(df=df_merged,cols=['major'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "This column has no major outliers and spread of data is actually good, however there is a large number of NONE present however as they are explicitly labled it will not be an issue later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_count = df_merged['education'].isin(['HIGH_SCHOOL', 'NONE']).sum()\n",
    "print(f\"Combined count for HIGH_SCHOOL & NONE: {combined_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "There seems to be a number of missing values from the major column that are true missing values and cannot be explained by missing or semantic information from another column. This however is not an issue as we are already assuming all types of missing data to be the same and then handled as NONE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_categoricals(df=df_merged,cols=['industry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "This is quite unexpected. It seems the one entry we removed i.e the president row was actually the president of the country as it seems the Government column is no longer in use. Meaning that our removal of the one row was all the more valid as a country only has one president therefore he/she would not have been at all representative of the majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'GOVERNMENT' from job_role if it exists\n",
    "df_merged['industry'] = df_merged['industry'].cat.remove_unused_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for numeric distribution plot\n",
    "df_numeric = df_merged[['years_experience', 'distance_from_cbd']]\n",
    "viz.plot_numeric_distribution(df=df_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "- years_experience column has good data spread as the boxes & whiskers are of equal length. It also has no outliers as there are no markers.\n",
    "\n",
    "- distance_from_cbd column has generally good data spread as the whiskers and boxes are of even length. However, there are 2 high value outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting distance_from_cbd outliers\n",
    "df_merged[df_merged['distance_from_cbd'] > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "- It seems there are only 2 of these instances and are too far from 75th quanitile + 1.5 x IQR as such we will just drop these 2 rows to ensure simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers from distance_from_cbd\n",
    "df_merged = df_merged[df_merged['distance_from_cbd'] <= 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "We will deal with salary_k next seperately. This is because there are very large value outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of salary_k\n",
    "df_merged['salary_k'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting salary_k == 0\n",
    "df_merged[df_merged['salary_k'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "It clearly seems that these values were wrongly keyed in, as these people all have jobs and some even have multiple years of experience. Therefore, these rows of data are clearly invalid entries. These will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping salary_k == 0 rows\n",
    "df_merged = df_merged[df_merged['salary_k'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of salary_k\n",
    "df_merged['salary_k'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "It also seems like there is a input of 10 million as well. Let's review the top 25 highest earners manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view top 25 salary_k\n",
    "top_25_salaries = df_merged.nlargest(25, 'salary_k')\n",
    "top_25_salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "Clearly, we can see that the 10000000k entry is incorrect as its the only one of its kind and needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 10000000k entry\n",
    "df_merged = df_merged[df_merged['salary_k'] != 10000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics of salary_k\n",
    "df_merged['salary_k'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "This has drastically reduced the range of values to normal human levels. Since we already reviewed the other 24 highest earners and they were genuine outliers, we cannot just haphazardly drop them. Therefore, lets verify the lowest 25 are also genuine outliers and if so let us log transform the values to reduce the skew of the data if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing bottom 25 salary_k\n",
    "bottom_25_salaries = df_merged.nsmallest(25, 'salary_k')\n",
    "bottom_25_salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "Yes this has confirmed my findings, all remaining salary_k values are valid and contain genuine outliers. Therefore, removing them is not required as they represent a portion of the job market. Lets, calculate the skew to verify if log transform is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % df_merged['salary_k'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "The spread of the data is acceptable and is within the range of skewness for a fairly symmetrical bell curve distribution of between -0.5 to 0.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "#### Analysing improbable situations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "Next now that the invalid entries have been removed and no longer affect the spread of data, we can now finally proceed to capping the extreme outliers this is to ensure that the final analysis is done on mostly generic data. We will also log how many rows we dropped before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobrole_salary_profile = (\n",
    "    df_merged.groupby('job_role', observed=True)['salary_k']\n",
    "    .agg(count='size',\n",
    "         lowest_salary='min',\n",
    "         median_salary='median',\n",
    "         highest_salary='max')\n",
    "    .reset_index()\n",
    "    .sort_values(by='median_salary', ascending=True)  # sort ascending for lowest pay\n",
    ")\n",
    "# Display the job role salary profile\n",
    "display(jobrole_salary_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "There is still quite a large range of values. One issue that stands out to me is the janitor job role. This is because other positions may have a possible scenario where the person may get paid that salary depending on what they do or the position. However, after some research to gain insight, I have come to the conclusion that the janitor role is very unlikely to have a salary of 189k even if they were a custodian engineer which is a type of janitor that is paid maximum ~145k lets plot some graphs to visualise this data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x=\"job_role\", y=\"salary_k\", data=df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "Upon further inspection, the Janitor job role is displaying suspicious salary values. As aforementioned, external research indicates that the highest-paid custodial roles (e.g., custodian engineers) earn around $149k. However, our dataset included Janitors earning salaries exceeding those of the median salary of CEOs, CFOs, and CTOs, which is highly unlikely. To address this, I will apply the well-established IQR rule (Q1 – 1.5×IQR, Q3 + 1.5×IQR) to the Janitor role only. This ensures we remove unrealistic outliers while preserving valid executive-level salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter Janitors and compute IQR for salary_k\n",
    "janitors = df_merged[df_merged['job_role'] == \"JANITOR\"].copy()\n",
    "\n",
    "# Compute Q1, Q3, and IQR for Janitors\n",
    "Q1 = janitors['salary_k'].quantile(0.25)\n",
    "Q3 = janitors['salary_k'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_limit = Q1 - 1.5 * IQR\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "print(\"Janitor Salary IQR Limits:\")\n",
    "print(\"Q1:\", Q1)\n",
    "print(\"Q3:\", Q3)\n",
    "print(\"Lower limit:\", lower_limit)\n",
    "print(\"Upper limit:\", upper_limit)\n",
    "\n",
    "# Apply the filter: keep only Janitors within bounds\n",
    "janitors_clean = janitors[\n",
    "    (janitors['salary_k'] >= lower_limit) &\n",
    "    (janitors['salary_k'] <= upper_limit)\n",
    "]\n",
    "\n",
    "# For other roles, keep everything as-is\n",
    "non_janitors = df_merged[df_merged['job_role'] != \"JANITOR\"]\n",
    "\n",
    "# Combine cleaned Janitors + other roles\n",
    "df_clean = pd.concat([janitors_clean, non_janitors], ignore_index=True)\n",
    "\n",
    "print(\"Before:\", len(df_merged))\n",
    "print(\"After cleaning Janitors:\", len(df_clean))\n",
    "print(\"Janitors removed:\", len(janitors) - len(janitors_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "From the cell output, the uppper limit is 137k which is a more realistic max value for the janitor role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "## What is the highest paying job for the web industry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "First lets filter out dataset to only include the web industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_web = df_clean[df_clean[\"industry\"] == \"WEB\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "Next I will group the respective jobs together and take the median value of them, we do this to get the median pay of each job. This is to ensure the average is not skewed by outliers as mean often times causes skewed data. We will also review the sample size of each job role to ensure each job role has sufficient representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by job_role in Web industry\n",
    "web_roles_stats = (df_web.groupby('job_role', observed=True)['salary_k'].agg(median_salary='median', count='size').sort_values(by='median_salary', ascending=False))\n",
    "top1_role_web = web_roles_stats.head(1)\n",
    "\n",
    "display(web_roles_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "Here we can see that the sample size for each job role is similar therefore we can compare the jobs without any issues. Lets plot the table to a bar chart to view the data with clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already grouped and calculated medians separately\n",
    "viz.plot_bar(df=web_roles_stats.reset_index(), x_col='job_role', y_col='median_salary',title=\"Median Salary by Job Role in Web Industry\")\n",
    "\n",
    "display(top1_role_web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "From the bar chart, we can clearly see that the job role with the highest median pay of 147k in the web industry is CEO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "##  Rank the top 10 jobs roles with the highest salary for all the industry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "Since this time we do not need to analyse a specific industry, we can procceed to the group by step immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by industry + job_role and compute median salary + counts\n",
    "role_salary_stats = (\n",
    "    df_clean.groupby(['industry', 'job_role'], observed=True)['salary_k']\n",
    "    .agg(median_salary='median', count='size')\n",
    "    .reset_index()\n",
    "    .sort_values(by='median_salary', ascending=False)\n",
    ")\n",
    "\n",
    "# Select the top 10 roles by median salary across all industries\n",
    "top10_roles_by_industry = role_salary_stats.head(10).copy()\n",
    "\n",
    "# Create new combined column\n",
    "top10_roles_by_industry['role_label'] = (\n",
    "    top10_roles_by_industry['industry'].astype(str) \n",
    "    + ' - ' + \n",
    "    top10_roles_by_industry['job_role'].astype(str)\n",
    ")\n",
    "\n",
    "# Drop the original two columns\n",
    "top10_roles_by_industry = top10_roles_by_industry.drop(columns=['industry', 'job_role'])\n",
    "\n",
    "# Reorder columns so role_label is first\n",
    "cols = ['role_label'] + [c for c in top10_roles_by_industry.columns if c != 'role_label']\n",
    "\n",
    "top10_roles_by_industry = top10_roles_by_industry[cols]\n",
    "\n",
    "top10_roles_by_industry.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_bar(df=top10_roles_by_industry.reset_index(), x_col='role_label', y_col='median_salary',title=\"Top 10 Job Roles across all industries by Median Salary\")\n",
    "\n",
    "display(top10_roles_by_industry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "These are the top 10 jobs ranked by median salary across all industries with the highest paid job roles on the left and the lowest on the right side. (Graphical view)\n",
    "\n",
    "These are the top 10 jobs ranked by median salary across all industries with the highest paid job roles at the top and the lowest at the bottom. (Table view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "## Which of the industries has the highest salary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "This time since we are only comparing across industries which contains a range of jobs, we need to have a robust method to compare the average salary of each industry. So what I will do is compare is highest lowest and median salary across all industries to get a more overarching view of the salaries of each type of individual i.e. median -> Typical employee, lowest -> lowest paying job-role & the highest -> best paying job-role in the industry. This should help the government understand which industries allow for growth of the employees and which industries have stagnant pay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each industry, get lowest, median, and highest job-role medians\n",
    "industry_salary_profile = (\n",
    "    df_clean.groupby('industry', observed=True)['salary_k']\n",
    "    .agg(count='size', lowest_salary='min', median_salary='median', highest_salary='max')\n",
    "    .reset_index()\n",
    "    .sort_values(by='median_salary', ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate delta (growth potential)\n",
    "industry_salary_profile['salary_delta'] = industry_salary_profile['highest_salary'] - industry_salary_profile['lowest_salary']\n",
    "industry_salary_profile.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_clustered_bars(\n",
    "    df=industry_salary_profile,\n",
    "    x_col='industry',\n",
    "    y_cols=['median_salary', 'highest_salary', 'lowest_salary','salary_delta'],\n",
    "    title=\"Median Salary, Highest Salary & Salary Delta by Industry\"\n",
    ")\n",
    "\n",
    "# Display the industry salary profile\n",
    "display(industry_salary_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "Here we can see that the sample size for each of the industries are very similar therefore we can compare without any additional steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "This clustered bar chart tells us quite a lot of information. We can observe that the typical employee in the Oil & Finance industry get paid the most at 128k. However, to differenciate between the two industries to find the \"highest\" paying we can then compare the highest pay, lowest pay & salary delta. Between the two industries, the oil industry has the highest salary at 301k as compared to the finance industries highest salary of 294k. Furthermore, when comparing the lowest salary, we can see that there is only a small difference of 1k but the oil industry is better paying at 37k as compared to the finance industry's 36k. When comparing the salary delta (Potential for growth) we can see that thee oil industry comes out ahead again at 264k compared to the finance industries 258k. Therefore, in conclusion typically for most employees, either the finance or oil industry will lead to the highest pay. However, if selecting a singular highest paying industry, the oil industry is the best paying industry when comparing all different levels of pay as well as the potential for income growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "## Which job has the lowest pay?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "As we are only comparing job roles this time, we will group by and only compare the job role salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each job_role, get count, min, median, max salary\n",
    "jobrole_salary_profile = (\n",
    "    df_clean.groupby('job_role', observed=True)['salary_k']\n",
    "    .agg(count='size',\n",
    "         lowest_salary='min',\n",
    "         median_salary='median',\n",
    "         highest_salary='max')\n",
    "    .reset_index()\n",
    "    .sort_values(by='median_salary', ascending=True)  # sort ascending for lowest pay\n",
    ")\n",
    "\n",
    "# The job with the lowest pay (first row)\n",
    "lowest_pay_job = jobrole_salary_profile.head(1)\n",
    "\n",
    "display(jobrole_salary_profile)  # full table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_clustered_bars(\n",
    "    df=jobrole_salary_profile,\n",
    "    x_col='job_role',\n",
    "    y_cols=['median_salary', 'highest_salary', 'lowest_salary'],\n",
    "    title=\"Median Salary, Highest Salary & Lowest Salary by Job Role\"\n",
    ")\n",
    "display(lowest_pay_job)          # just the lowest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "From the graph and by sorting the df, we can see that the lowest paying job is the janitor role. It is the lowest paying across all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "## Which industries have the lowest pay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each industry, get count, min, median, max salary\n",
    "industry_salary_profile = (\n",
    "    df_clean.groupby('industry', observed=True)['salary_k']\n",
    "    .agg(count='size',\n",
    "         lowest_salary='min',\n",
    "         median_salary='median',\n",
    "         highest_salary='max')\n",
    "    .reset_index()\n",
    "    .sort_values(by='median_salary', ascending=True)  # sort ascending for lowest pay\n",
    ")\n",
    "\n",
    "# The industry with the lowest pay (first row)\n",
    "lowest_pay_industry = industry_salary_profile.head(1)\n",
    "\n",
    "display(industry_salary_profile)  # full table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.plot_clustered_bars(\n",
    "    df=industry_salary_profile,\n",
    "    x_col='industry',\n",
    "    y_cols=['median_salary', 'highest_salary', 'lowest_salary'],\n",
    "    title=\"Median Salary, Highest Salary & Lowest Salary by Industry\"\n",
    ")\n",
    "display(lowest_pay_job)          # just the lowest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "The lowest paying industry is the education industry. It has the lowest min, median and maximum pay among all industries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {},
   "source": [
    "## Given that the median salary per year is $114,000\n",
    "- Which industry has the highest percentage of people who are below the median salary?\n",
    "- What are the job roles that are below the median salary? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "### Which industry has the highest percentage of people who are below the median salary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the cleaned DataFrame for filtering\n",
    "df_industry_filtered = df_clean.copy()\n",
    "\n",
    "# Fliter the rows by salary_k < 114\n",
    "df_industry_filtered['below_median'] = df_clean['salary_k'] < 114\n",
    "\n",
    "# Group by industry and agg the count & sum\n",
    "industry_below = (\n",
    "    df_industry_filtered.groupby(\"industry\")['below_median']\n",
    "    .agg(['count','sum'])  # count = total, sum = below median\n",
    ")\n",
    "\n",
    "# Calculate the percentage of people below median salary in each industry\n",
    "industry_below['%_below_median'] = ((industry_below['sum'] / industry_below['count']) * 100).round(2)\n",
    "\n",
    "# Sort descending to find industry with highest % below median\n",
    "industry_below_sorted = industry_below.sort_values(by=\"%_below_median\", ascending=False)\n",
    "\n",
    "display(industry_below_sorted.head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "The industry with the largest % of people below the median pay is the education industry with 66.88%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "### What are the job roles that are below the median salary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "I am assuming that the question is asking for the count & % as all the job roles are under the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the cleaned DataFrame for filtering\n",
    "df_job_filtered = df_clean.copy()\n",
    "\n",
    "# Filter rows below the median\n",
    "df_job_filtered['below_median'] = df_clean['salary_k'] < 114\n",
    "\n",
    "# Group by job role: total count and number below median\n",
    "role_below = (\n",
    "    df_job_filtered.groupby(\"job_role\")['below_median']\n",
    "    .agg(['count','sum'])   # count = total rows, sum = rows below median\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate percentage\n",
    "role_below['%_below_median'] = ((role_below['sum'] / role_below['count']) * 100).round(2)\n",
    "\n",
    "# Sort by percentage (descending)\n",
    "role_below = role_below.sort_values(by='%_below_median', ascending=False).reset_index(drop=True)\n",
    "\n",
    "display(role_below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "These are the job roles below the mediian salary as well as the % of the people with the job role who are under the median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "## Determine if there is a relationship between years of experience and salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "Since these are two numeric columns, we can easily plot a scatter graph to determine the trend visually without the need of statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.scatterplot(x=\"years_experience\", y=\"salary_k\", data=df_clean, alpha=0.3)\n",
    "plt.title(\"Years of Experience vs. Salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {},
   "source": [
    "The graph is very noisy and as such it is difficult to determine a trend or relationship due to the overlapping points. However, it does seem that there is a slight increase in the max salary as age increases however there is no irrefutable proof. Therefore, to prevent this current situation where the min and max points for each year is very far apart, I will take the median age of each age and also add in a best fit line to determine the type of relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate median salary for each years_experience (across all roles/industries)\n",
    "median_salary_overall = (\n",
    "    df_clean.groupby(\"years_experience\")['salary_k']\n",
    "    .median()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Scatter the median points\n",
    "sns.scatterplot(\n",
    "    x=\"years_experience\", y=\"salary_k\",\n",
    "    data=median_salary_overall,\n",
    "    marker=\"o\", s=60, color=\"blue\", label=\"Median Salary\"\n",
    ")\n",
    "\n",
    "# Add regression line based only on medians\n",
    "sns.regplot(\n",
    "    x=\"years_experience\", y=\"salary_k\",\n",
    "    data=median_salary_overall,\n",
    "    scatter=False,          \n",
    "    ci=None,                \n",
    "    color=\"red\", \n",
    "    line_kws={\"lw\":2, \"alpha\":0.8}, \n",
    "    label=\"Best Fit Line\"\n",
    ")\n",
    "\n",
    "plt.title(\"Median Salary vs. Years of Experience (Best Fit Line)\")\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Median Salary (K)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# X = years of experience, y = median salary\n",
    "X = median_salary_overall['years_experience']\n",
    "y = median_salary_overall['salary_k']\n",
    "\n",
    "# Fit a straight line (degree=1)\n",
    "slope, intercept = np.polyfit(X, y, 1)\n",
    "\n",
    "print(f\"Intercept: {intercept:.2f}\")\n",
    "print(f\"Slope (gradient): {slope:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {},
   "source": [
    "From the graph of median salary vs years of experience that has a best fitline, we can determine there is a direct relationship between the two variables. As the years of experience, the median salary increases by 1.94k. The starting median base pay at 0 years of experience being ~91K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "## Is there a relationship between education and salary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {},
   "source": [
    "This is slightly different as education is an ordinal categorical column and salary is a numeric column so to determine the relationship we need to either encode the labels in order least to most educated or we can use a barchart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the different unique values in education\n",
    "education_counts = df_clean['education'].value_counts(dropna=False)\n",
    "display(education_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define natural order\n",
    "edu_order = {\n",
    "    \"NONE\": 0,\n",
    "    \"HIGH_SCHOOL\": 1,\n",
    "    \"BACHELORS\": 2,\n",
    "    \"MASTERS\": 3,\n",
    "    \"DOCTORAL\": 4\n",
    "}\n",
    "\n",
    "# Map to ordered integers\n",
    "df_clean['education_encoded'] = df_clean['education'].map(edu_order)\n",
    "\n",
    "# Spearman correlation\n",
    "corr = df_clean['education_encoded'].corr(df_clean['salary_k'], method='spearman')\n",
    "print(\"Spearman correlation:\", round(corr, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "The Spearman correlation between education and salary was 0.388, which indicates a weak-to-moderate positive relationship. This suggests that while higher education is associated with higher salaries, other factors such as job role, industry, and years of experience play a larger role in determining compensation. Let us verify the statistical analysis with a visual graph to ensure this is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median salary per education group\n",
    "edu_salary = (\n",
    "    df_clean.groupby(\"education\")['salary_k']\n",
    "    .median()\n",
    "    .reindex([\"NONE\",\"HIGH_SCHOOL\",\"BACHELORS\",\"MASTERS\",\"DOCTORAL\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=\"education\", y=\"salary_k\", data=edu_salary, palette=\"viridis\")\n",
    "\n",
    "plt.title(\"Median Salary by Education Level\")\n",
    "plt.xlabel(\"Education Level\")\n",
    "plt.ylabel(\"Median Salary (in $000s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "The bar chart of median salaries by education level shows a clear upward trend: employees with higher education levels generally earn more. The largest salary increase occurs between High School/None and Bachelor’s degree holders. However, while the correlation is positive, the progression is not uniform — salaries for Master’s and Doctoral levels are closer together. This visual evidence supports the Spearman correlation of 0.388, confirming a weak-to-moderate positive relationship between education and salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156",
   "metadata": {},
   "source": [
    "## Does the major they studied affect the salary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "This is once again a different situation, this is becuase there is no clear order to the major column as its nominal. Therefore, we have to visually confirm if any specific major leads to higher pay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute median salary per major and sort descending\n",
    "major_order = (\n",
    "    df_clean.groupby(\"major\")['salary_k']\n",
    "    .median()\n",
    "    .sort_values(ascending=False)\n",
    "    .index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the boxplot and barplot side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18,6), sharey=True)\n",
    "\n",
    "# --- Barplot: Median salary per major (ranked) ---\n",
    "sns.barplot(\n",
    "    x=\"major\", y=\"salary_k\",\n",
    "    data=df_clean,\n",
    "    estimator=\"median\", palette=\"viridis\",\n",
    "    order=major_order,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Median Salary by Major (Ranked)\")\n",
    "axes[0].set_xlabel(\"Major\")\n",
    "axes[0].set_ylabel(\"Salary (k)\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# --- Boxplot: Salary distribution per major (ranked) ---\n",
    "sns.boxplot(\n",
    "    x=\"major\", y=\"salary_k\",\n",
    "    data=df_clean,\n",
    "    palette=\"Set2\",\n",
    "    order=major_order,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Salary Distribution by Major (Ranked)\")\n",
    "axes[1].set_xlabel(\"Major\")\n",
    "axes[1].set_ylabel(\"\")  # hide duplicate y-label\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160",
   "metadata": {},
   "source": [
    "After analyzing the relationship between major and salary using both ranked barplots of the median and boxplots of the distribution. The ranked barplot revealed that majors such as Engineering, Business, and Math had the highest median salaries, while majors such as Biology, Literature, and None were positioned towards the lower end. However, the boxplot showed that there was substantial overlap in salary distributions across majors, with many individuals in lower-ranked majors earning salaries comparable to those in higher-ranked majors. From this, I determined that while major does have an influence on salary, the effect is not strong enough to be considered a decisive factor on its own, and other variables such as job role, industry, and experience must also be taken into account."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
